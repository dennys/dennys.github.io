[{"body":"","link":"https://dennys.github.io/en/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://dennys.github.io/en/","section":"","tags":null,"title":"Dennys Diary"},{"body":"","link":"https://dennys.github.io/en/doc/","section":"doc","tags":null,"title":"Docs"},{"body":"","link":"https://dennys.github.io/en/categories/it/","section":"categories","tags":null,"title":"IT"},{"body":"","link":"https://dennys.github.io/en/tags/network/","section":"tags","tags":null,"title":"Network"},{"body":"Requirement Procedures Generate key\nExecute ssh-keygen and generate the file id_rsa\n1C:\\Users\\dennys\u0026gt;ssh-keygen 2Generating public/private rsa key pair. 3Enter file in which to save the key (C:\\Users\\dennys/.ssh/id_rsa): The content of id_rsa is like the following.\n1ssh-rsa AAAAB******************== dennys@mypc123 Put the key on Linux\nOpen the file ~/.ssh/authorized_keys and paste the content of id_rsa into it. Test automatically login\nExecute ssh name@host, you don't need to input password any more. Configuratino Windows Terminal\nOpen settings.json of Windows Terminal.\nClick Windows Terminal -\u0026gt; Settings -\u0026gt; Open JSON file\nAdd a new profile in settings.json\n1 \u0026#34;profiles\u0026#34;: 2 { 3 \u0026#34;defaults\u0026#34;: {}, 4 \u0026#34;list\u0026#34;: 5 [ 6 { 7 \u0026#34;name\u0026#34;: \u0026#34;ssh linux100\u0026#34;, 8 \u0026#34;commandline\u0026#34;: \u0026#34;ssh dennys@192.168.1.100\u0026#34;, 9 \u0026#34;tabTitle\u0026#34;: \u0026#34;This is a tab title\u0026#34;, 10 \u0026#34;hidden\u0026#34;: false 11 }, 12 ] 13 } Test the new profile, when you click it, Windows Terminal should open a new tab and login automatically.\n","link":"https://dennys.github.io/en/doc/network/ssh-login-automatically-windows-terminal/","section":"doc","tags":["Network"],"title":"Never Type Your Password Again: Automatic SSH Logins in Windows Terminal"},{"body":"","link":"https://dennys.github.io/en/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://dennys.github.io/en/tags/gitlab/","section":"tags","tags":null,"title":"GitLab"},{"body":"","link":"https://dennys.github.io/en/series/gitlab-devops/","section":"series","tags":null,"title":"GitLab DevOps"},{"body":"Purpose In the fast-evolving landscape of software development, ensuring code quality and security is paramount. To streamline this process, integrating a robust set of tools becomes imperative. In this blog, we embark on a journey to seamlessly integrate GitLab, SonarQube, pytest, coverage.py, and dependency-check into a unified ecosystem. By combining version control, code analysis, testing, code coverage evaluation, and dependency scanning, we aim to create a powerful and efficient pipeline that not only enhances the overall quality of the codebase but also fortifies it against potential vulnerabilities. Join us as we explore each tool's role in this integration, demonstrating how they collectively contribute to a more resilient and high-performing software development lifecycle.\nRequirements After the code is committed, these reports should be generated by CI/CD pipelines automatically.\nTest result and test coverage report by Jest. Dependency check report by Dependency-Check Code quality by SonarQube Sommarize these reports into GitLab \u0026amp; SonarQube. Procedures Test result and coverage Test result and coverage in console and HTML reports (Jest) Test result (Console):\nI don't find a Jest feature to generate plain text test result on console, please let me know if you know how to do it.\nTest result (HTML):\nTest coverage (Console):\nTest coverage (HTML):\nTest Result and coverage on GitLab I don't try it, since GitLab only supports JUnit format, you can use jest-junit to generate JUnit format report and upload it into artifacts of GitLab.\nConfiguration files package.json package.json\n1{ 2 \u0026#34;name\u0026#34;: \u0026#34;test1\u0026#34;, 3 \u0026#34;version\u0026#34;: \u0026#34;1.0.7\u0026#34;, 4 \u0026#34;main\u0026#34;: \u0026#34;server.js\u0026#34;, 5 \u0026#34;dependencies\u0026#34;: { 6 \u0026#34;fastify\u0026#34;: \u0026#34;^4.24.2\u0026#34;, 7 \u0026#34;fastify-plugin\u0026#34;: \u0026#34;^4.5.1\u0026#34;, 8 \u0026#34;mysql2\u0026#34;: \u0026#34;^3.6.2\u0026#34;, 9 \u0026#34;winston\u0026#34;: \u0026#34;^3.11.0\u0026#34;, 10 \u0026#34;winston-daily-rotate-file\u0026#34;: \u0026#34;^4.7.1\u0026#34; 11 }, 12 \u0026#34;devDependencies\u0026#34;: { 13 \u0026#34;chai\u0026#34;: \u0026#34;^4.3.10\u0026#34;, 14 \u0026#34;chai-http\u0026#34;: \u0026#34;^4.4.0\u0026#34;, 15 \u0026#34;jest\u0026#34;: \u0026#34;29.7.0\u0026#34;, 16 \u0026#34;jest-html-reporters\u0026#34;: \u0026#34;^3.1.4\u0026#34; 17 }, 18 \u0026#34;scripts\u0026#34;: { 19 \u0026#34;start\u0026#34;: \u0026#34;node server\u0026#34;, 20 \u0026#34;testonly\u0026#34;: \u0026#34;jest\u0026#34;, 21 \u0026#34;test\u0026#34;: \u0026#34;jest --coverage \u0026#34; 22 } 23} jest.config.json jest.config.json\n1{ 2 \u0026#34;reporters\u0026#34;: [ 3 \u0026#34;default\u0026#34;, 4 [ 5 \u0026#34;jest-html-reporters\u0026#34;, 6 { 7 \u0026#34;publicPath\u0026#34;: \u0026#34;test-report\u0026#34;, 8 \u0026#34;pageTitle\u0026#34;: \u0026#34;Test1 Test Report\u0026#34; 9 } 10 ] 11 ] 12} Dockerfile Dockerfile\n1FROM node:20.6.1-bookworm-slim 2 3WORKDIR /app 4ADD . /app 5RUN ls 6EXPOSE 8001 7CMD [\u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34;] .gitlab-ci.yml .gitlab-ci.yml\n1variables: 2 PROJECT_NAME: \u0026#34;test_1\u0026#34; 3 VER: 1.0.7 4 DOCKER_FILE_NAME: test_1 5 SONARQUBE_URL: http://example.com. 6 SONARQUBE_TOKEN: squ_**************************************** 7 SONARQUBE_PROJECT_KEY: test_1 8 9image: node:20.6.1-bookworm-slim 10 11stages: 12 - setup 13 - test 14 - dependency_check 15 - quality_check 16 - build_docker 17 18cache: 19 paths: 20 - node_modules/ 21 22setup: 23 stage: setup 24 script: 25 - npm install 26 27test: 28 stage: test 29 script: 30 - npm test 31 artifacts: 32 paths: 33 - ./test-report 34 - ./coverage 35 36dependency_check: 37 allow_failure: true 38 stage: dependency_check 39 image: 40 #name: registry.gitlab.com/gitlab-ci-utils/docker-dependency-check:latest 41 #name: owasp/dependency-check:latest 42 name: owasp/dependency-check-action:latest 43 entrypoint: [\u0026#34;\u0026#34;] 44 script: 45 - - /usr/share/dependency-check/bin/dependency-check.sh --scan \u0026#34;./\u0026#34; --format ALL --project \u0026#34;$PROJECT_NAME\u0026#34; --failOnCVSS 0 46 artifacts: 47 when: always 48 paths: 49 - \u0026#34;./dependency-check-report.html\u0026#34; 50 - \u0026#34;./dependency-check-report.json\u0026#34; 51 52quality_check: 53 stage: quality_check 54 image: 55 name: sonarsource/sonar-scanner-cli:5.0.1 56 script: 57 - sonar-scanner -D\u0026#34;sonar.host.url=$SONARQUBE_URL\u0026#34; 58 -D\u0026#34;sonar.token=$SONARQUBE_TOKEN\u0026#34; 59 -D\u0026#34;sonar.projectKey=$SONARQUBE_PROJECT_KEY\u0026#34; 60 -D\u0026#34;sonar.projectName=$PROJECT_NAME\u0026#34; 61 -D\u0026#34;sonar.sourceEncoding=utf-8\u0026#34; 62 -D\u0026#34;sonar.exclusions=test/**/*, coverage/**/*, test-report/**/*, dependency-check-report.html\u0026#34; 63 -D\u0026#34;sonar.javascript.lcov.reportPaths=./coverage/lcov.info\u0026#34; 64 -D\u0026#34;sonar.dependencyCheck.jsonReportPath=./dependency-check-report.json\u0026#34; 65 -D\u0026#34;sonar.dependencyCheck.htmlReportPath=./dependency-check-report.html\u0026#34; 66 67build_docker: 68 stage: build_docker 69 image: docker 70 services: 71 - name: docker:dind 72 alias: thedockerhost 73 variables: 74 # Tell docker CLI how to talk to Docker daemon; see 75 # https://docs.gitlab.com/ee/ci/docker/using_docker_build.html#use-docker-in-docker-executor 76 DOCKER_HOST: tcp://thedockerhost:2375/ 77 # Use the overlayfs driver for improved performance: 78 DOCKER_DRIVER: overlay2 79 DOCKER_TLS_CERTDIR: \u0026#34;\u0026#34; 80 script: 81 - echo Buile Ver $VER 82 - docker build --no-cache -t $DOCKER_FILE_NAME:$VER -f Dockerfile . 83 - docker save $DOCKER_FILE_NAME:$VER -o $DOCKER_FILE_NAME-$VER.tar 84 artifacts: 85 paths: 86 - ./$DOCKER_FILE_NAME-$VER.tar 87 only: 88 - master ","link":"https://dennys.github.io/en/doc/devops/gitlab-nodejs-dependency-test-sonarqube/","section":"doc","tags":["GitLab","SonarQube","JavaScript"],"title":"GitLab Node.js SonarQube Integration"},{"body":"","link":"https://dennys.github.io/en/tags/javascript/","section":"tags","tags":null,"title":"JavaScript"},{"body":"","link":"https://dennys.github.io/en/series/","section":"series","tags":null,"title":"Series"},{"body":"","link":"https://dennys.github.io/en/tags/sonarqube/","section":"tags","tags":null,"title":"SonarQube"},{"body":"","link":"https://dennys.github.io/en/tags/python/","section":"tags","tags":null,"title":"Python"},{"body":"Purpose In the fast-evolving landscape of software development, ensuring code quality and security is paramount. To streamline this process, integrating a robust set of tools becomes imperative. In this blog, we embark on a journey to seamlessly integrate GitLab, SonarQube, pytest, coverage.py, and dependency-check into a unified ecosystem. By combining version control, code analysis, testing, code coverage evaluation, and dependency scanning, we aim to create a powerful and efficient pipeline that not only enhances the overall quality of the codebase but also fortifies it against potential vulnerabilities. Join us as we explore each tool's role in this integration, demonstrating how they collectively contribute to a more resilient and high-performing software development lifecycle.\nRequirements After the code is committed, these reports should be generated by CI/CD pipelines automatically.\nTest result and test coverage report by pytest and Coverage.py. Dependency check report by Dependency-Check Code quality by SonarQube Sommarize these reports into GitLab \u0026amp; SonarQube. Procedures Test result and coverage Test result and coverage in console and HTML reports (Pytest and Coverage.py) Test result (Console):\nYou can add -rAR in pyproject.toml to show test result on console.\nTest result (HTML):\nI don't try it, you can try the following 2 tools. The 1st is provided by pytest but it seems simple. The 2nd seems beautifuler, but the last commit date is 2022/04/29.\nhttps://github.com/pytest-dev/pytest-html https://github.com/prashanth-sams/pytest-html-reporter Test coverage (Console):\nYou can add --cov-report=term (term means terminal) in pyproject.toml to show test coverage on console.\nTest coverage (HTML):\nYou can add --cov-report=html in pyproject.toml to generate test coverage HTML report.\nTest Result and coverage on GitLab If you want to show test results in GitLab, you need to add --junitxml=junit_report.xml (GitLab supports JUnit format) and --cov-report=xml (GitLab supports Cobertura format.) in pyproject.toml to ask pytest to generate corresponding xml files.\n1[tool.pytest.ini_options] 2python_files = [\u0026#34;tests/*.py\u0026#34;] 3addopts = \u0026#34;-R --cov=. --cov-report=term --cov-report=xml --cov-report=html --junitxml=junit_report.xml\u0026#34; Then modify .gitlab-ci.yml to upload JUnit and Coverage XML files into GitLab artifacts.\n1artifacts: 2 reports: 3 junit: junit_report.xml 4 coverage_report: # coverage_report is supported in GitLab 14.10 5 coverage_format: cobertura 6 path: coverage.xml When you run GitLab CI/CD pipelines, you can see the following log in pytest stage.\n1- generated xml file: /builds/-i39jfYQ/0/ai/service_test/junit_report.xml - 2... 3Uploading artifacts... 4junit_report.xml: found 1 matching artifact files and directories 5Uploading artifacts as \u0026#34;junit\u0026#34; to coordinator... 201 Created id=3169 responseStatus=201 Created token=******** After the GitLab CI/CD pipeline is completed, you can see the test summary.\nAnd you can click the job to see the detail.\nTest Result and coverage on SonarQube SonarQube can show the test results in old version, but I'm not sure when SonarQube removed it. You can see test coverage only on SonarQube now.\nYou need to add --cov-report=xml in pyproject.toml to generate coverage.xml\nYou need to add -D\u0026quot;sonar.python.coverage.reportPaths=./coverage.xml\u0026quot; in .gitlab-ci.yml to ask SonarQube to load coverage.xml into SonarQube.\nWhen you run GitLab CI/CD pipelines, you can see the following logs in pytest stage.\n1Coverage XML written to file coverage.xml And you can see the coverage report in SonarQube now.\nDependency Check There are several docker images, please use the first one, or it will download all CVE updates (from 2002 to now) every time. (reference: https://hub.docker.com/r/owasp/dependency-check-action/)\nhttps://hub.docker.com/r/owasp/dependency-check-action/ https://hub.docker.com/r/owasp/dependency-check https://registry.gitlab.com/gitlab-ci-utils/docker-dependency-check Because Python is still in experientmal, please add --enableExperimental parameter.\nPlease add -n in the parameter to stop downloading updates, or you will see the following log, it takes several minutes to execute.\n1... 2[INFO] Checking for updates 3[INFO] NVD CVE requires several updates; this could take a couple of minutes. 4[INFO] Download Started for NVD CVE - 2002 5[INFO] Download Complete for NVD CVE - 2002 (1841 ms) 6[INFO] Processing Started for NVD CVE - 2002 7[INFO] Download Started for NVD CVE - 2003 8[INFO] Download Complete for NVD CVE - 2003 (1425 ms) 9[INFO] Processing Started for NVD CVE - 2003 10... 11[INFO] Download Started for NVD CVE - 2023 12[INFO] Download Complete for NVD CVE - 2023 (2710 ms) 13[INFO] Processing Started for NVD CVE - 2023 14... 15[INFO] Updated the CPE ecosystem on 136030 NVD records 16... Reports (on GitLab)\nDepends on your requirement, you can put dependency check result on GitLab CI/CD pipelines, HTML reports, or SonarQube.\nIf there is a security vulnerability, you can see it in GitLab CI/CD pipeline logs, for example, I use bootstrap 3.2 and it shows the error.\n1[ERROR] 2One or more dependencies were identified with vulnerabilities that have a CVSS score greater than or equal to \u0026#39;0.0\u0026#39;: 3bootstrap.min.js: Bootstrap before 4.0.0 is end-of-life and no longer maintained.(3.9) 4requirements.txt: CVE-2022-25882(7.5) If you want to download HTML report, you can add dependency-check-report.html in artifacts of .gitlab-ci.yml.\n1artifacts: 2 when: always 3 paths: 4 - \u0026#34;./dependency-check-report.html\u0026#34; Reports (on SonarQube)\nYou need to install Dependency-Check Plugin for SonarQube\nAdd dependency-check-report.json, sonar.dependencyCheck.jsonReportPath, and sonar.dependencyCheck.htmlReportPath in .gitlab-ci.yml.\n1artifacts: 2 when: always 3 paths: 4 - \u0026#34;./dependency-check-report.html\u0026#34; 5 - \u0026#34;./dependency-check-report.json\u0026#34; 1-D\u0026#34;sonar.dependencyCheck.jsonReportPath=../dependency-check-report.json\u0026#34; 2-D\u0026#34;sonar.dependencyCheck.htmlReportPath=../dependency-check-report.html\u0026#34; The plugin will insert security vulnerabilities into SonarQube's result, you can see the detail in the issues.\nIf you want to see the HTML report in SonarQube, you can click the More tab and see it.\nCode quality (SonarQube) The dashboard in SonarQube 10.0 is a little different than the previous version.\nSonarQube:\nConfiguration files pyproject.toml The following is the completed pyproject.toml\n1[tool.pytest.ini_options] 2python_files = [\u0026#34;tests/*.py\u0026#34;] 3addopts = \u0026#34;-rAR --cov=. --cov-report=term --cov-report=xml --cov-report=html --junitxml=junit_report.xml\u0026#34; 4log_cli = true 5log_cli_level = \u0026#34;INFO\u0026#34; 6 7[tool.coverage.run] 8branch = true 9omit = [ 10 \u0026#34;*/tests/*\u0026#34;, 11 \u0026#34;config.py\u0026#34;, # it seems a bug, refer https://github.com/nedbat/coveragepy/issues/1653 12 \u0026#34;config-3.py\u0026#34;, 13] 14 15[tool.coverage.paths] 16source = [\u0026#34;.\u0026#34;] 17 18[tool.coverage.html] 19directory = \u0026#34;coverage_html_report\u0026#34; .gitlab-ci.yml The following is the completed .gitlab-ci.yml\n1variables: 2 PROJECT_NAME: \u0026#34;Service_test\u0026#34; 3 VER: 1.0.0 4 DOCKER_FILE_NAME: service_test 5 SONARQUBE_URL: https://example.com:9000/ 6 SONARQUBE_TOKEN: squ_**************************************** 7 SONARQUBE_PROJECT_KEY: service_test 8 PIP_CACHE_DIR: \u0026#34;$CI_PROJECT_DIR/.cache.pip\u0026#34; 9 10image: python:3.11.6 11 12cache: 13 paths: 14 - .cache/pip 15 - venv/ 16 17stages: 18 - test 19 - dependency_check 20 - quality_check 21 22test: 23 stage: test 24 script: 25 # Show basic information 26 - python --version 27 - pip list 28 29 # Prepare virtual environment 30 - pip install --upgrade pip 31 - pip install virtualenv 32 - virtualenv venv 33 - source venv/bin/activate 34 35 # Install libraries 36 - pip install -r requirements.txt 37 - pip list 38 39 # Test 40 - pytest --version 41 - pytest 42 artifacts: 43 reports: 44 junit: junit_report.xml 45 coverage_report: # coverage_report is supported after GitLab 14.10 46 coverage_format: cobertura 47 path: coverage.xml 48 49 paths: 50 - coverage_html_report 51 - coverage.xml # Save coverage XML report as an artifact 52 53dependency_check: 54 stage: dependency_check 55 image: 56 #name: registry.gitlab.com/gitlab-ci-utils/docker-dependency-check:latest 57 name: owasp/dependency-check-action:latest 58 entrypoint: [\u0026#34;\u0026#34;] 59 script: 60 - - /usr/share/dependency-check/bin/dependency-check.sh -n --scan \u0026#34;.\u0026#34; --format ALL --project \u0026#34;$PROJECT_NAME\u0026#34; --failOnCVSS 0 --enableExperimental --log ./dependency-check.log 61 artifacts: 62 when: always 63 paths: 64 - \u0026#34;./dependency-check-report.html\u0026#34; 65 - \u0026#34;./dependency-check-report.json\u0026#34; 66 - \u0026#34;./dependency-check.log\u0026#34; 67 68quality_check: 69 stage: quality_check 70 image: 71 name: sonarsource/sonar-scanner-cli:5.0.1 72 script: 73 - echo $PROJECT_NAME 74 - echo $SONARQUBE_URL 75 - echo $SONARQUBE_TOKEN 76 - sonar-scanner -D\u0026#34;sonar.host.url=$SONARQUBE_URL\u0026#34; 77 -D\u0026#34;sonar.token=$SONARQUBE_TOKEN\u0026#34; 78 -D\u0026#34;sonar.projectKey=$SONARQUBE_PROJECT_KEY\u0026#34; 79 -D\u0026#34;sonar.projectName=$PROJECT_NAME\u0026#34; 80 -D\u0026#34;sonar.sourceEncoding=utf-8\u0026#34; 81 -D\u0026#34;sonar.exclusions=test/**/*, coverage/**/*, test-report/**/*, dependency-check-report.html\u0026#34; 82 -D\u0026#34;sonar.javascript.lcov.reportPaths=./coverage/lcov.info\u0026#34; 83 -D\u0026#34;sonar.python.coverage.reportPaths=./coverage.xml\u0026#34; 84 -D\u0026#34;sonar.dependencyCheck.jsonReportPath=../dependency-check-report.json\u0026#34; 85 -D\u0026#34;sonar.dependencyCheck.htmlReportPath=../dependency-check-report.html\u0026#34; 86 -D\u0026#34;sonar.python.version=3.8,3.11\u0026#34; ","link":"https://dennys.github.io/en/doc/devops/python-quality-gitlab-sonarqube-owasp-dependency/","section":"doc","tags":["GitLab","Python","SonarQube"],"title":"Python Code Quality: GitLab, SonarQube, OWASP, Dependency-Check, and Essential Dev Tools Integration"},{"body":"","link":"https://dennys.github.io/en/tags/node/","section":"tags","tags":null,"title":"Node"},{"body":"","link":"https://dennys.github.io/en/series/node/","section":"series","tags":null,"title":"Node"},{"body":"Requirement I'm tring to write some Node.js functions to wrap some basic database operations and my requirement is to support these features.\nConnection pool Prepared statement Transaction Throw exception: when there is any exception, it can throw the exception to outside. Node.js MySQL drivers First, I find there are 4 drivers for Node.js to connect to MySQL/MariaDB, this is really ...\nThis is an old project and it's original driver is mysql. Because it doesn't support Prepared statement, I decide to migrate it to mysql2. These are MySQL drivers for Node.js:\nmysql mysql2 MySQL Connector/Node.js by Oracle MariaDB Node.js connector by MariaDB Code sample The folowing code will declare the driver and create the connection pool.\n1\u0026#39;use strict\u0026#39; 2 3const fs = require(\u0026#39;fs/promises\u0026#39;); // Node.js built-in module for file operations 4const mysql = require(\u0026#39;mysql2\u0026#39;); // Database client driver 5const winston = require(\u0026#39;winston\u0026#39;); // Log framework 6const winstonConfig = require(\u0026#39;./winston-config.js\u0026#39;); // Load the JSON config 7 8const logger = winston.createLogger(winstonConfig); 9 10let pool; 11 12async function initDatabase() { 13 logger.info(\u0026#34;Start to connect to database...\u0026#34;); 14 try { 15 pool = require(\u0026#39;mysql2/promise\u0026#39;).createPool({ 16 decimalNumbers: true, // This is important, check https://github.com/sidorares/node-mysql2/tree/master/documentation/en 17 host: process.env.DB_HOST, 18 port: process.env.DB_PORT, 19 user: process.env.DB_USER, 20 password: process.env.DB_PASSWORD, 21 database: process.env.DB_DATABASE, 22 waitForConnections: true, 23 connectionLimit: 10, 24 maxIdle: 10, // max idle connections, the default value is the same as `connectionLimit` 25 idleTimeout: 60000, // idle connections timeout, in milliseconds, the default value 60000 26 queueLimit: 0, 27 enableKeepAlive: true, 28 keepAliveInitialDelay: 0, 29 timezone: \u0026#34;Z\u0026#34; 30 }); 31 32 } catch (err) { 33 logger.error(\u0026#34;Cannot connect to database: \u0026#34; + err); 34 process.exit(1) 35 } 36} Query sample\n1async function getTable1(value) { 2 logger.info(\u0026#34;Start to get data from table1\u0026#34;); 3 let result = []; 4 let sql = `SELECT * FROM table1 WHERE key = ?\u0026#39; : \u0026#39;\u0026#39;}`; 5 logger.info(`SQL: ${sql}, bind var: ${value}`); 6 7 let conn = null; 8 try { 9 conn = await pool.getConnection(); 10 const [response] = await conn.execute(sql, [value]); 11 for (let row of response) { 12 let row1 = { 13 \u0026#34;key1\u0026#34;: row.VALUE1, 14 \u0026#34;key2\u0026#34;: row.VALUE2, 15 \u0026#34;key3\u0026#34;: row.VALUE3, 16 }; 17 result.push(row1); 18 } 19 } catch (err) { 20 logger.error(\u0026#34;Get data error: \u0026#34;, err); 21 throw err; 22 } finally { 23 if (conn) await conn.release(); // Release connection 24 } 25 return result; 26} (Do NOT use) Query sample\nThe following code can query data, but if there is any exception, it will NOT raise. I'm not sure the reason, it seems due to promise? 1async function getTable1(value) { 2 logger.info(\u0026#34;Start to get data from table1\u0026#34;); 3 let result = []; 4 let sql = `SELECT * FROM table1 WHERE key = ?\u0026#39; : \u0026#39;\u0026#39;}`; 5 logger.info(`SQL: ${sql}, bind var: ${value}`); 6 7 await pool.getConnection() 8 .then(conn =\u0026gt; { 9 const res = conn.query(sql, [value]); 10 conn.release(); 11 return res; 12 }).then(result =\u0026gt; { 13 for (let row of result[0]) { 14 let row1 = { 15 \u0026#34;key1\u0026#34;: row.VALUE1, 16 \u0026#34;key2\u0026#34;: row.VALUE2, 17 \u0026#34;key3\u0026#34;: row.VALUE3, 18 }; 19 result.push(row1); 20 } 21 }).catch(err =\u0026gt; { 22 logger.error(\u0026#34;Get data error: \u0026#34;, err); 23 throw err; 24 }); 25 return result; 26} Insert data sample\n1async function postTable1(data) { 2 logger.info(\u0026#34;Start to insert data into table1\u0026#34;); 3 let sql = \u0026#34;INSERT INTO table1 (...) VALUES (?,?,?, ...);\u0026#34;; 4 let bindVar = [data.value1, data.value2, data.value3, ...]; 5 logger.info(`SQL: ${sql}, bind var: ${bindVar}`); 6 7 let insert_id = -1; 8 let conn = null; 9 try { 10 conn = await pool.getConnection(); 11 const [response] = await conn.execute(sql, bindVar); 12 insert_id += response.insertId; 13 logger.info(`Insert to table1, the new id is ${insert_id}`); 14 } catch (err) { 15 logger.error(\u0026#34;Insert table1 error: \u0026#34;, err); 16 throw err; 17 } finally { 18 if (conn) await conn.release(); // Release connection 19 } 20 return insert_id; 21} Update sample\n1async function updateTable1(data) { 2 logger.info(\u0026#34;Start to update name of table1\u0026#34;); 3 let sql = \u0026#34;UPDATE table1 SET name = ? WHERE id = ? \u0026#34; 4 let bindVar = [data.name, data.id]; 5 logger.info(`SQL: ${sql}, bind var: ${JSON.stringify(bindVar)}`); 6 let affectedRows = 0; 7 8 let conn = null; 9 try { 10 conn = await pool.getConnection(); 11 const [response] = await conn.execute(sql, bindVar); 12 affectedRows += response.affectedRows; 13 logger.info(`Update ${affectedRows} row(s) in table1.`); 14 } catch (err) { 15 logger.error(\u0026#34;Update name of table1 error: \u0026#34;, err); 16 throw err; 17 } finally { 18 if (conn) await conn.release(); // Release connection 19 } 20 return affectedRows; 21} Delete sample\n1async function deleteTable(data) { 2 logger.info(`Start to delete record of table1, input is ${JSON.stringify(data)}`); 3 let sql = \u0026#34;DELETE FROM table1 WHERE id = ?; \u0026#34; 4 let bindVar = [data.id]; 5 logger.info(`SQL: ${sql}, bind var: ${JSON.stringify(bindVar)}`); 6 let affectedRows = 0; 7 8 let conn = null; 9 try { 10 conn = await pool.getConnection(); 11 const [responseTestResult] = await conn.execute(sql, bindVar); 12 affectedRows += responseTestResult.affectedRows; 13 logger.info(`Delete ${affectedRows} row(s) in table1.)`); 14 } catch (err) { 15 logger.error(\u0026#34;Delete table1 error: \u0026#34;, err); 16 throw err; 17 } finally { 18 if (conn) await conn.release(); // Release connection 19 } 20 return t_affectedRows; 21} Transaction\n1async function updateMultipleTable(data) { 2 logger.info(`Start to update table1 and table2, input is ${JSON.stringify(data)}`); 3 let affectedRows = 0; 4 let sql1 = \u0026#34;UPDATE table1 SET name = ? WHERE id = ? \u0026#34; 5 let bindVar1 = [data.name1, data.id1]; 6 let sql2 = \u0026#34;UPDATE table2 SET name = ? WHERE id = ? \u0026#34; 7 let bindVar2 = [data.name2, data.id2]; 8 9 let conn = null; 10 try { 11 conn = await pool.getConnection(); 12 await conn.beginTransaction(); 13 14 // Update Table1 15 const [response] = await conn.execute(sql1, bindVar1); 16 affectedRows += response.affectedRows; 17 logger.info(`Update ${affectedRows} row(s) in table1.`); 18 19 // Update Table2 20 const [response] = await conn.execute(sql2, bindVar2); 21 affectedRows += response.affectedRows; 22 logger.info(`Update ${affectedRows} row(s) in table2.`); 23 24 // Commit 25 await conn.commit(); 26 } catch (error) { 27 logger.error(\u0026#34;Update table1 error: \u0026#34;, err); 28 if (conn) await conn.rollback(); // Rollback for any error 29 throw error; 30 } finally { 31 if (conn) await conn.release(); // Release connection 32 } 33 return affectedRows; 34} ","link":"https://dennys.github.io/en/doc/javascript/node-mysql/","section":"doc","tags":["Node","JavaScript"],"title":"Node.js + MySQL (Connecton Pool + Prepared Statement + Transaction)"},{"body":"","link":"https://dennys.github.io/en/tags/.net/","section":"tags","tags":null,"title":".NET"},{"body":"","link":"https://dennys.github.io/en/tags/ci/cd/","section":"tags","tags":null,"title":"CI/CD"},{"body":"Requirement We know how to run SonarQube to scan .NET 4 code in GitLab + SonarQube + .NET 4.8 (non-docker), but it needs a Windows environment and does not support Linux.\nTherefore, we want to find a solution to run SonarQube on Linux in docker if possible. In this artiflce, we try to use Mono framework (a cross platform .NET 4.7 framework) to run SonarQube on Linux to scan .NET 4 code.\nProcedures Download the docker image from official hub: https://hub.docker.com/_/mono, you can choose your preferred version.\n1$ docker pull mono:6.12.0.182 Use this command to enter the bash of the docker.\n1$ docker run -v /home/xxx:/tmp/host -it mono bash Install the environment; you need Java (to run SonarQube), wget (to download SonarQube) and unzip (to extract zip files)\n1$ apt-get update \u0026amp;\u0026amp; apt-get install -y openjdk-11-jdk wget unzip 2$ mkdir scanner 3$ cd scanner Install Sonar Scanner for .NET\n1$ wget https://github.com/SonarSource/sonar-scanner-msbuild/releases/download/5.15.0.80890/sonar-scanner-msbuild-5.15.0.80890-net46.zip 2$ unzip sonar-scanner-msbuild-5.15.0.80890-net46.zip Make Sonar Scanner executable (ref: https://stackoverflow.com/a/48657295/489517).\n1$ chmod +x sonar-scanner-4.8.1.3023/bin/sonar-scanner Execute Sonar Scanner to scan code.\n1$ mono /scanner/SonarScanner.MSBuild.exe begin /k:xxx /name:xxx \\ 2 /d:sonar.token=\u0026#34;squ_*****************************\u0026#34; 3 /d:sonar.host.url=\u0026#34;http://localhost:9000/\u0026#34; 4$ msbuild project1.sln 5$ mono /scanner/SonarScanner.MSBuild.exe end /d:sonar.token=\u0026#34;squ_*****************************\u0026#34;\u0026#34; Then you can see the log on the console like:\n1Microsoft (R) Build Engine version 16.10.1 for Mono 2Copyright (C) Microsoft Corporation. All rights reserved. 3 4Building the projects in this solution one at a time. To enable parallel build, please add the \u0026#34;-m\u0026#34; switch. 5Build started 12/04/2023 03:41:58. 6Project \u0026#34;/tmp/p1/project1.sln\u0026#34; on node 1 (default targets). 7... Connect to http://localhost:9000/ and you should see your project.\nKnown issues Becasuse I use Mono to build code, this solution cannot support a feature is not provided by Mono (e.g. WPF)\nYou can reference https://www.mono-project.com/docs/about-mono/compatibility/\nI use Java 11 to do it, but Java 11 support will be dropped by SonarQube in Jan 2024. (https://community.sonarsource.com/t/96597)\nThe path of .resx should be lower case.\nBecause MSBuild assumes the file paths are case-insensitive, but the file paths are NOT case-insensitive on Linux. If you use uppercase characters in your .resx file paths, MSBuild cannot find it on Linux.\nReference:\nhttps://stackoverflow.com/a/60973376/489517 https://developercommunity.visualstudio.com/t/managed-resources-editor-writes-relative-file-path/856303#T-N863464 And it's fixed in VS 2022 17.6 in 2023. But because MSBuild of Mono 6.12 is still 16.10, you need to wait Mono team to ugprade it.\n[ERROR] FATAL UNHANDLED EXCEPTION: System.ComponentModel.Win32Exception (0x80004005): ... Native error= Access denied\nPlease check the article above, you need to run chmod +x sonar-scanner-4.8.1.3023/bin/sonar-scanner\nError MSB3103: Invalid Resx file. AND data length expected 512, read 0\nI'm not sure about the reason, I find a similar case but no solutino. It's for some .ico files, I changed these ico files to other images and it works. https://github.com/dotnet/msbuild/issues/2838\n","link":"https://dennys.github.io/en/doc/devops/sonarqube-mono-dotnet4-integration/","section":"doc","tags":["SonarQube",".NET","CI/CD"],"title":"Run SonarQube on Linux Docker with Mono to scan .NET 4.8 Code"},{"body":"","link":"https://dennys.github.io/en/tags/microbit/","section":"tags","tags":null,"title":"Micro:bit"},{"body":"","link":"https://dennys.github.io/en/series/microbit/","section":"series","tags":null,"title":"Micro:bit"},{"body":"\rWarning\rThis code only supports Micro:bit v2 or above. Demo Code Library Because I use Expansion Board, please add Neopixel extension in your micro:bit editor (https://makecode.microbit.org/pkg/microsoft/pxt-neopixel) Block code This is the block code, you can click Edit button to check the code.\nPython code If you prefer Python, the following is the code. (I use block only to develop this.)\n1run = 0 2item: neopixel.Strip = None 3 4def on_button_pressed_a(): 5 global run, item 6 run = 1 7 while run == 1: 8 item = neopixel.create(DigitalPin.P2, 24, NeoPixelMode.RGB) 9 item.show_rainbow(1, 360) 10 for index in range(30): 11 item.show() 12 item.rotate(1) 13 basic.pause(50) 14input.on_button_pressed(Button.A, on_button_pressed_a) 15 16def on_button_pressed_b(): 17 global run 18 item.show_color(neopixel.colors(NeoPixelColors.BLACK)) 19 item.clear() 20 run = 0 21input.on_button_pressed(Button.B, on_button_pressed_b) 22 23def on_forever(): 24 pass 25basic.forever(on_forever) Q\u0026amp;A If you get error code 207\nIf you get error code 207 during downloading, it means you are using Micro:bit V1 (You can reference error code from https://makecode.microbit.org/device/error-codes)\nReference https://github.com/YahboomTechnology/RGB-LED-Circular-expansion-board https://www.circuspi.com/index.php/2019/08/15/mooncarlesson3/ (Chinese) https://www.elecfreaks.net/learn-cn/microbitKit/experiment_box/experiment_box_case_10.html (Chinese) Where to buy https://category.yahboom.net/products/rgbledb Taiwan: https://www.taiwansensor.com.tw/product/microbit-%E7%92%B0%E5%BD%A2-rgb-%E7%87%88%E6%93%B4%E5%B1%95%E6%9D%BF/ ","link":"https://dennys.github.io/en/doc/microbit/microbit-rainbow-lighting/","section":"doc","tags":["Micro:bit"],"title":"Micro:bit Rainbow Lighting (with Expansion Board)"},{"body":"Requirement I aim to develop a [Node].js(https://nodejs.org/)-based FastAPI that is OpenAPI compatible (formerly known as Swagger). To accomplish this, I will employ RepiDoc for generating OpenAPI documents and keep track of the API's execution status using swagger-stats.\nCode This is package.json\n1{ 2 \u0026#34;name\u0026#34;: \u0026#34;fastify-example\u0026#34;, 3 \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, 4 \u0026#34;description\u0026#34;: \u0026#34;A simple Fastify example with OpenAPI\u0026#34;, 5 \u0026#34;type\u0026#34;: \u0026#34;commonjs\u0026#34;, 6 \u0026#34;scripts\u0026#34;: { 7 \u0026#34;start\u0026#34;: \u0026#34;node app.js\u0026#34; 8 }, 9 \u0026#34;dependencies\u0026#34;: { 10 \u0026#34;@fastify/express\u0026#34;: \u0026#34;^2.3.0\u0026#34;, 11 \u0026#34;@fastify/static\u0026#34;: \u0026#34;^6.11.2\u0026#34;, 12 \u0026#34;@fastify/swagger\u0026#34;: \u0026#34;^8.10.1\u0026#34;, 13 \u0026#34;@fastify/swagger-ui\u0026#34;: \u0026#34;^1.9.3\u0026#34;, 14 \u0026#34;fastify\u0026#34;: \u0026#34;latest\u0026#34;, 15 \u0026#34;swagger-stats\u0026#34;: \u0026#34;^0.99.7\u0026#34; 16 } 17} This is app.js\nIf you want to use Swagger UI, you can remark line 40-53 and line 55-69\n1\u0026#39;use strict\u0026#39; 2 3const fastify = require(\u0026#39;fastify\u0026#39;)({ 4logger: true 5}); 6 7// Static plugin to render HTML pages 8const path = require(\u0026#39;node:path\u0026#39;) 9fastify.register(require(\u0026#39;@fastify/static\u0026#39;), { 10root: path.join(__dirname, \u0026#39;/public\u0026#39;), 11prefix: \u0026#39;/public/\u0026#39;, // optional: default \u0026#39;/\u0026#39; 12// http://localhost:3000/public/index.html 13// constraints: { host: \u0026#39;example.com\u0026#39; } // optional: default {} 14}) 15 16// Swagger Stats 17const swStats = require(\u0026#39;swagger-stats\u0026#39;); 18const apiSpec = require(\u0026#39;./swagger.json\u0026#39;); 19 20// Register Swagger 21const registerSwagger = async () =\u0026gt; { 22try { 23 await fastify.register(require(\u0026#39;@fastify/swagger\u0026#39;), { 24 // openapi options 25 openapi: { 26 openapi: \u0026#34;3.1.0\u0026#34;, 27 info: { 28 title: \u0026#39;test openapi\u0026#39;, 29 description: \u0026#39;this is a test\u0026#39;, 30 version: \u0026#39;1.0.1\u0026#39;, 31 }, 32 // externalDocs: Object, 33 // servers: [ Object ], 34 // components: Object, 35 // security: [ Object ], 36 // tags: [ Object ] 37 }, 38 39 // Swagger options 40 swagger: { 41 info: { 42 title: \u0026#39;Test swagger\u0026#39;, 43 description: \u0026#39;Testing the Fastify swagger API\u0026#39;, 44 version: \u0026#39;0.1.0\u0026#39; 45 }, 46 externalDocs: { 47 url: \u0026#39;https://swagger.io\u0026#39;, 48 description: \u0026#39;Find more info here\u0026#39; 49 }, 50 host: \u0026#39;localhost\u0026#39;, 51 schemes: [\u0026#39;http\u0026#39;], 52 } 53 }) 54 55 // await fastify.register(require(\u0026#39;@fastify/swagger-ui\u0026#39;), { 56 // routePrefix: \u0026#39;/documentation\u0026#39;, 57 // uiConfig: { 58 // docExpansion: \u0026#39;full\u0026#39;, 59 // deepLinking: false 60 // }, 61 // uiHooks: { 62 // onRequest: function (request, reply, next) { next() }, 63 // preHandler: function (request, reply, next) { next() } 64 // }, 65 // staticCSP: true, 66 // transformStaticCSP: (header) =\u0026gt; header, 67 // transformSpecification: (swaggerObject, request, reply) =\u0026gt; { return swaggerObject }, 68 // transformSpecificationClone: true 69 // }) 70} catch (err) { 71 console.log(err); 72 process.exit(1); 73} 74} 75 76// Register Fastify Route 77const registerRoute = async () =\u0026gt; { 78try { 79 // Define a sample route 80 fastify.get(\u0026#39;/route1\u0026#39;, async (request, reply) =\u0026gt; { 81 return { hello: \u0026#39;world\u0026#39; }; 82 }); 83 84 // Define a sample route 85 fastify.get(\u0026#39;/route2\u0026#39;, async (request, reply) =\u0026gt; { 86 return { hello: \u0026#39;world\u0026#39; }; 87 }); 88 89 // Define a swagger route 90 fastify.get(\u0026#39;/doc\u0026#39;, async (request, reply) =\u0026gt; { 91 return fastify.swagger(); 92 // reply.send(fastify.swagger()) ==\u0026gt; work, but bad, why? 93 }); 94 95 // Enable swagger-stats 96 // fastify.register(require(\u0026#39;fastify-express\u0026#39;)).then(()=\u0026gt;{ 97 fastify.register(require(\u0026#39;@fastify/express\u0026#39;)).then(()=\u0026gt;{ 98 fastify.register(swStats.getFastifyPlugin, {swaggerSpec:apiSpec}); 99 }); 100 101 await fastify.ready() 102 await fastify.listen({ port: 3000 }); 103 104 console.log(`Server listening on ${fastify.server.address().port}`); 105} catch (err) { 106 console.log(err); 107 process.exit(1); 108} 109}; 110 111const main = async () =\u0026gt; { 112console.log(\u0026#34;Start to register Swagger\u0026#34;) 113await registerSwagger(); 114console.log(\u0026#34;Start to register Route\u0026#34;) 115await registerRoute(); 116console.log(\u0026#34;Done\u0026#34;) 117} 118 119main(); If you want to use RapiDoc, you need to add a HTML file\n1\u0026lt;!doctype html\u0026gt; \u0026lt;!-- Important: must specify --\u0026gt; 2\u0026lt;html\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;!-- Important: rapi-doc uses utf8 characters --\u0026gt; 5 \u0026lt;script type=\u0026#34;module\u0026#34; src=\u0026#34;rapidoc-min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 6\u0026lt;/head\u0026gt; 7\u0026lt;body\u0026gt; 8 \u0026lt;rapi-doc spec-url = \u0026#34;http://localhost:3000/doc\u0026#34;\u0026gt; \u0026lt;/rapi-doc\u0026gt; 9\u0026lt;/body\u0026gt; 10\u0026lt;/html\u0026gt; Result OpenAPI json: http://localhost:3000/doc RapiDoc: http://localhost:3000/public/index.html Swagger UI: http://localhost:3000/documentation/ Swagger statistics: http://localhost:3000/swagger-stats/ What's next Consider to try Elements\n","link":"https://dennys.github.io/en/doc/javascript/node-openapi/","section":"doc","tags":["Node","JavaScript"],"title":"Node + Fastify + OpenAPI + Statistics"},{"body":"","link":"https://dennys.github.io/en/tags/dotnet/","section":"tags","tags":null,"title":"DotNet"},{"body":"","link":"https://dennys.github.io/en/series/dotnet/","section":"series","tags":null,"title":"DotNet"},{"body":"Reqruiement I have two view models - MainViewModel and OptionViewModel -, that share a common component CustomConfiguration. My goal is to load the data into MainViewModel in MainWindow, and when the Option button ic clicked on MainWindow, OptionWindow will be opened, and the CustomConfiguration data will be passed into it.\nCode MainWindow.xaml Only bind a data (CustomConfig.GeneralSettings.IsLogEnabled) and assign the command.\n1\u0026lt;Window.DataContext\u0026gt; 2 \u0026lt;local:MainViewModel /\u0026gt; 3\u0026lt;/Window.DataContext\u0026gt; 4\u0026lt;StackPanel\u0026gt; 5 \u0026lt;TextBlock Text=\u0026#34;{Binding CustomConfig.GeneralSettings.IsLogEnabled}\u0026#34; /\u0026gt; 6 \u0026lt;Button Content=\u0026#34;Open Options\u0026#34; Command=\u0026#34;{Binding OpenOptionsCommand}\u0026#34; /\u0026gt; 7\u0026lt;/StackPanel\u0026gt; OptionWindow.xaml The same as MainWindow.xaml, only bind a data (CustomConfig.GeneralSettings.IsLogEnabled) and assign the command.\n1\u0026lt;Window.DataContext\u0026gt; 2 \u0026lt;local:OptionViewModel /\u0026gt; 3\u0026lt;/Window.DataContext\u0026gt; 4\u0026lt;StackPanel\u0026gt; 5 \u0026lt;CheckBox Content=\u0026#34;Enable Logging\u0026#34; IsChecked=\u0026#34;{Binding CustomConfig.GeneralSettings.IsLogEnabled}\u0026#34; /\u0026gt; 6 \u0026lt;Button Content=\u0026#34;OK\u0026#34; Command=\u0026#34;{Binding OKCommand}\u0026#34; /\u0026gt; 7\u0026lt;/StackPanel\u0026gt; MainWindow.xaml.cs No special commands here.\n1public partial class MainWindow : Window { 2 public MainWindow() { 3 InitializeComponent(); 4 } 5} OptionWindow.xaml.cs No special commands here.\n1public partial class OptionWindow : Window { 2 public OptionWindow() { 3 InitializeComponent(); 4 } 5} OptionWindow.xaml.cs =\u0026gt; If you want to clone data. In the previous version, the CustomConfiguration instance was passed from MainViewModel into OptionViewModel, so they share one instance. If you want to create a copy of CustomConfiguration in OptionViewModel, you can use this constructor.\n1 public OptionWindow(CustomConfiguration config) { 2 InitializeComponent(); 3 4 //_initialConfig = config; 5 _viewModel = new OptionViewModel(config.Clone()); 6 DataContext = _viewModel; 7 } CustomConfiguration.cs 1public class CustomConfiguration : INotifyPropertyChanged { 2 private GeneralSettingsClass _generalSettings = new GeneralSettingsClass(); 3 4 public GeneralSettingsClass GeneralSettings { 5 get =\u0026gt; _generalSettings; 6 set { 7 _generalSettings = value; 8 OnPropertyChanged(); 9 } 10 } 11 12 public event PropertyChangedEventHandler PropertyChanged; 13 protected virtual void OnPropertyChanged([CallerMemberName] string propertyName = null) { 14 PropertyChanged?.Invoke(this, new PropertyChangedEventArgs(propertyName)); 15 } 16} 17 18public class GeneralSettingsClass : INotifyPropertyChanged { 19 private bool _isLogEnabled = false; 20 21 public bool IsLogEnabled { 22 get =\u0026gt; _isLogEnabled; 23 set { 24 _isLogEnabled = value; 25 OnPropertyChanged(); 26 } 27 } 28 29 public event PropertyChangedEventHandler PropertyChanged; 30 protected virtual void OnPropertyChanged([CallerMemberName] string propertyName = null) { 31 PropertyChanged?.Invoke(this, new PropertyChangedEventArgs(propertyName)); 32 } 33} MainViewModel.cs Please remember to check the Shared variable, if you want to share one instance of CustomConfiguration, you can set it to true. If you want to create a copy of CustomConfiguation, you can set it to false.\n1public class MainViewModel : INotifyPropertyChanged { 2 public CustomConfiguration CustomConfig { get; } = new CustomConfiguration(); 3 4 public RelayCommand OpenOptionsCommand { get; } 5 6 public MainViewModel() { 7 OpenOptionsCommand = new RelayCommand(OpenOptionsCommandExecute); 8 } 9 10 private void OpenOptionsCommandExecute() { 11 bool Shared = false; 12 if (Shared) { 13 // This ViewModel will be shared 14 var optionViewModel = new OptionViewModel(CustomConfig); 15 var optionWindow = new OptionWindow() { DataContext = optionViewModel }; 16 optionWindow.ShowDialog(); 17 } else { 18 // This ViewModel will be cloned 19 var optionWindow = new OptionWindow(CustomConfig.Clone()); 20 optionWindow.ApplyChanges += (s, config) =\u0026gt; { 21 this.CustomConfig = config; 22 }; 23 optionWindow.ShowDialog(); 24 } 25 } 26 27 public event PropertyChangedEventHandler PropertyChanged; 28 protected virtual void OnPropertyChanged([CallerMemberName] string propertyName = null) { 29 PropertyChanged?.Invoke(this, new PropertyChangedEventArgs(propertyName)); 30 } 31} OptionViewModel.cs 1public class OptionViewModel : INotifyPropertyChanged { 2 private CustomConfiguration _customConfig; 3 4 public CustomConfiguration CustomConfig { 5 get =\u0026gt; _customConfig; 6 set { 7 _customConfig = value; 8 OnPropertyChanged(); 9 } 10 } 11 12 public RelayCommand OKCommand { get; } 13 14 public OptionViewModel(CustomConfiguration customConfig) { 15 CustomConfig = customConfig; 16 OKCommand = new RelayCommand(OKCommandExecute); 17 } 18 19 private void OKCommandExecute() { 20 CustomConfig.Save(); 21 // Close the option window 22 (Application.Current.MainWindow as Window)?.Close(); 23 } 24 25 public event PropertyChangedEventHandler PropertyChanged; 26 protected virtual void OnPropertyChanged([CallerMemberName] string propertyName = null) { 27 PropertyChanged?.Invoke(this, new PropertyChangedEventArgs(propertyName)); 28 } 29} Test Case 1: share one instance Run the program. Click OpenOptions button. Click Enable Logging check box. You shuold see the flag in MainWindow is switched. Case 2: Not share one instance (create a copy) Set Shared to false in MainViewModel.cs. Run the program. Click OpenOptions button. Click Enable Logging check box. You shuold see the flag in MainWindow is NOT changed. ","link":"https://dennys.github.io/en/doc/dotnet/sharing-components-between-two-view-models-in-csharp-mvvm/","section":"doc","tags":["dotNet"],"title":"Sharing components between two view models in C# MVVM"},{"body":"","link":"https://dennys.github.io/en/tags/google-photos/","section":"tags","tags":null,"title":"Google Photos"},{"body":"I'm trying to search my photos in Google Photos by device name (iPhone or Google Pixel or ...), but it's weird. I have a Google Pixel 5, when I try to search for Google Pixel 5, I can get all the photos taken by it, but when I just type Google Pixel to search, it only shows partial photos. I'm not sure about Google's search algorithm, I just record my test results.\nApple iPhone 6 Apple iPhone 11 Apple iPhone XS Max Apple iPad (6th generation) Google Pixel 5 Samsung SM-M135F (Samsung Galaxy M13) Samsung SM-G935F (Samsung S7 edge) Panasonic DMC-G6 Panasonic DMC-G2 SONY DSC-RX100M5A By the way, you can use Apple, Google, and Samsung to find their devices, but it will search for some other photos. For example, if your photo is a Samsung refrigerator, it will be included in the search results.\nReference:\nhttps://support.google.com/photos/thread/368298/how-to-search-google-photos-by-which-device-took-the-picture?hl=en ","link":"https://dennys.github.io/en/doc/software/google-photos-search-by-device/","section":"doc","tags":["Software","Google Photos"],"title":"Search Google Photos by Device"},{"body":"","link":"https://dennys.github.io/en/tags/software/","section":"tags","tags":null,"title":"Software"},{"body":"Requirement Try to find software that can search or replace text in files, and it should support these functions:\nFree software. Supports UTF-8. It can show partial find content in the search result (or it takes a long time to see the file content). Double click can open the file with the associated editor. Supports replace. As fast as possible (of course) Software comparison Compare 3 software, I prefer to use grepWin.\ngrepWin SearchMyFiles Find and Replace (FNR) grepWin SearchMyFiles Find and Replace (FNR) GitHub star 1.3K N/A N/A UTF-8 Yes Yes (*R1) Yes Show file content Yes (in a column) Yes (in a column) Yes (in another panel) Double click open file Yes Yes (*R2) Yes Replace Yes Yes Yes Speed Fast Fast R1: SearchMyFiles' UTF-8 support is strange, if you try to search a UTF-8 word (e.g. a Chinese word), it can search and display the content. However, if you search for an English word (but the content has some other Chinese words), it will display garbled in the content. R2: The default behavior of double click in SearchMyFiles is to open a window to show file properties, you can change the associated editor using Options -\u0026gt; Double-Click. ","link":"https://dennys.github.io/en/doc/software/file-search-and-replace-tool/","section":"doc","tags":["Software"],"title":"File Search/Replace Tool"},{"body":"","link":"https://dennys.github.io/en/tags/android/","section":"tags","tags":null,"title":"Android"},{"body":"","link":"https://dennys.github.io/en/series/android-%E4%BD%BF%E7%94%A8%E5%B0%8F%E6%8A%80%E5%B7%A7/","section":"series","tags":null,"title":"Android 使用小技巧"},{"body":"Requirement In Gmail's default notification actions, you can choose to Archive or Reply . But you can change it.\nSolution Open the Gmail App's General setting, change the *Default email notification action\u0026quot;. The default action is Archive, but you can change it to Delete. After configuring, you will be able to delete emails in Gmail's notification. What's next It's better to enable both Archive and Delete in the notification.\n","link":"https://dennys.github.io/en/doc/android/gmail-notify-delete-archive/","section":"doc","tags":["Android"],"title":"How to Change the Gmail Notification Action from Archive to Delete"},{"body":"Requirement In the previous article, we know we can change archive to delete in the Email(Gmail) notification of Android (Reference: http://dennys.github.io/en/doc/android/gmail-notify-delete-archive/) But we cannot enable BOTH, therefore, I try to find a solution in other Apps.\nK-9 Mail, ( K-9 Mail Joins The Thunderbird Family ) FairEmail Gmail Let's check Gmail's ability, the default action is Archive. Open the Gmail App's General setting, and change the *Default email notification action\u0026quot;. The default action is Archive, you can change it to Delete. After the configuration, you can delete mail in Gmail's notification. K9 K9 offers the actions Reply, Mark Read, Delete actions, but doesn't offer the action Archive. FairEmail In FairEmail, the default actions are Trash Can and Mark Read You can select Notification actions in the Settings, and you can select at most 3 actions in the free version. ","link":"https://dennys.github.io/en/doc/android/android-gmail-notification-actions/","section":"doc","tags":["Android"],"title":"Android Gmail Notification Actions (Archive, Delete, ...)"},{"body":"If you want to use SonarQube to scan your code during GitLab's CI/CD flow, please see the following procedures.\nGitLab + SonarQube + .NET Core or .NET 5/6/7/... (docker) Refer to this document to integrate GitLab and SonarQube.\nSet the environment variable in GitLab\n$SONAR_URL: The URL of SonarQube $SONAR_TOKEN: The token to access SonarQube $CI_PROJECT_DIR: This is a predifined variable in GitLab, you can reference https://docs.gitlab.com/ee/ci/variables/predefined_variables.html $CI_JOB_NAME: This is a predifined variable in GitLab, you can reference https://docs.gitlab.com/ee/ci/variables/predefined_variables.html Add the following to your .gitlab-ci.yml\n1variables: 2 SONAR_PROJECT_KEY: This-is-my-project-key-in-SonarQube 3 4owasp_dependency_check: 5 image: 6 name: registry.gitlab.com/gitlab-ci-utils/docker-dependency-check:latest 7 entrypoint: [\u0026#34;\u0026#34;] 8 stage: dependency_check 9 tags: 10 - docker 11 script: 12 # Job will scan the project root folder and fail if any vulnerabilities with CVSS \u0026gt; 0 are found 13 - /usr/share/dependency-check/bin/dependency-check.sh --scan \u0026#34;./\u0026#34; --format ALL --project \u0026#34;$CI_PROJECT_NAME\u0026#34; --failOnCVSS 0 14 # Dependency Check will only fail the job based on CVSS scores, and in some cases vulnerabilities do not 15 # have CVSS scores (e.g. those from NPM audit), so they don\u0026#39;t cause failure. To fail for any vulnerabilities 16 # grep the resulting report for any \u0026#34;vulnerabilities\u0026#34; sections and exit if any are found (count \u0026gt; 0). 17 - if [ $(grep -c \u0026#34;vulnerabilities\u0026#34; dependency-check-report.json) -gt 0 ]; then exit 2; fi 18 allow_failure: true 19 artifacts: 20 when: always 21 paths: 22 # Save the HTML and JSON report artifacts 23 - \u0026#34;./dependency-check-report.html\u0026#34; 24 - \u0026#34;./dependency-check-report.json\u0026#34; 25 26sonarqube: 27 allow_failure: true 28 stage: sonar 29 tags: 30 - docker 31 image: 32 name: sonarsource/sonar-scanner-cli:latest 33 entrypoint: [\u0026#34;\u0026#34;] 34 variables: 35 SONAR_HOST_URL: $SONAR_URL 36 SONAR_USER_HOME: \u0026#34;${CI_PROJECT_DIR}/.sonar\u0026#34; # Defines the location of the analysis task cache 37 #SONAR_LOGIN: $SONAR_TOKEN 38 GIT_DEPTH: \u0026#34;0\u0026#34; # Tells git to fetch all the branches of the project, required by the analysis task 39 cache: 40 key: \u0026#34;${CI_JOB_NAME}\u0026#34; 41 paths: 42 - .sonar/cache 43 script: 44 - echo $SONAR_URL 45 - echo $CI_COMMIT_BRANCH 46 - sonar-scanner -D sonar.login=\u0026#34;$SONAR_TOKEN\u0026#34; -D sonar.projectKey=$SONAR_PROJECT_KEY -Dsonar.projectName=\u0026#34;${SONAR_PROJECT_KEY} (${CI_COMMIT_BRANCH})\u0026#34; 47 -D sonar.dependencyCheck.jsonReportPath=\u0026#34;./dependency-check-report.json\u0026#34; -D sonar.dependencyCheck.htmlReportPath=\u0026#34;./dependency-check-report.html\u0026#34; 48 49security-code-scan: 50 stage: security-scan 51 tags: 52 - docker 53 allow_failure: true 54 image: mcr.microsoft.com/dotnet/sdk:5.0 55 #image: mcr.microsoft.com/dotnet/core/sdk:3.1 56 script: 57 - echo $env:Path 58 - dotnet restore 59 - dotnet tool install --global security-scan 60 - mkdir report 61 - $HOME/.dotnet/tools/security-scan geosense_netcore_project_template.sln --excl-proj=**/*Test*/** --export=report/out.sarif 62 artifacts: 63 paths: 64 - ./geosense_netcore_project_template/report Try to build the code\nGitLab + SonarQube + .NET 4.8 (non-docker) The docker support of .NET 4.0 or below is not very good, if you cannot upgrade, I suggest you use non-docker solution. You can refer to http://dennys.github.io/en/doc/devops/gitlab-dotnet4-ci-cd/ to build .NET 4 applications in GitLab CI/CD flow. And you can follow the following procedures to integrate SonarQube.\nReference this document to integrate GitLab and SonarQube. You need to install these softwares: Java, you can choose Adopt JDK or others, please add java.exe to PATH. Sonar Scanner for .NET, assumes you put it in C:\\Gitlab\\sonar-scanner. Add the following to your .gitlab-ci.yml 1sonarqube_windows: 2 allow_failure: true 3 tags: 4 - windows 5 stage: sonar 6 variables: 7 SONAR_PROJECT_KEY: ProjectXXX 8 SONAR_TOKEN: **************************************** 9 script: 10 - echo $SONAR_URL 11 - echo $CI_COMMIT_BRANCH 12 - dotnet tool update --global dotnet-sonarscanner 13 - dotnet C:\\Gitlab\\sonar-scanner\\SonarScanner.MSBuild.dll begin /k:$SONAR_PROJECT_KEY /d:sonar.host.url=\u0026#34;$SONAR_URL\u0026#34; /d:sonar.login=\u0026#34;$SONAR_TOKEN\u0026#34; /d:sonar.scm.provider=git 14 - dotnet restore ******.sln 15 - dotnet build ******.sln 16 - dotnet C:\\Gitlab\\sonar-scanner\\SonarScanner.MSBuild.dll end /d:sonar.login=\u0026#34;$SONAR_TOKEN\u0026#34; Try to build the code Run SonarQube on Linux Docker with Mono to scan .NET 4.8 Code If you want to run SonarQube to scan .Net 4 code on Linux docker, you can reference Run SonarQube on Linux Docker with Mono to scan .NET 4.8 Code, but it has some limitation.\n","link":"https://dennys.github.io/en/doc/devops/gitlab-sonarqube-integration-dotnet/","section":"doc","tags":["GitLab","SonarQube",".NET","CI/CD"],"title":"GitLab SonarQube Integration with .NET"},{"body":"","link":"https://dennys.github.io/en/tags/prometheus/","section":"tags","tags":null,"title":"Prometheus"},{"body":" Requirement I have 3 Prometheus servers and want to consolidate all of them into 1 Grafana. And one VM (Server C) is in a restricted zone and this zone only allows a special firewall rule from this zone to other zones.\nPrometheus A pulls (receives) metrics from Prometheus B In this scenario, we use the Federation feature to pull metrics from another Prometheus. You don't need to make any changes to Prometheus B, just add a new job to prometheus.yml of Prometheus A.\nPlease check lines 13~15, you can only pull some metrics you need.\n1# prometheus.yml of A 2 - job_name: \u0026#39;federate\u0026#39; 3 scrape_interval: 15s 4 honor_labels: true 5 metric_relabel_configs: 6 - source_labels: [id] 7 regex: \u0026#39;^static-agent$\u0026#39; 8 action: drop 9 metrics_path: \u0026#39;/federate\u0026#39; 10 params: 11 \u0026#39;match[]\u0026#39;: 12 #- \u0026#39;{job=\u0026#34;prometheus\u0026#34;}\u0026#39; 13 #- \u0026#39;{__name__=~\u0026#34;job:.*\u0026#34;}\u0026#39; 14 - \u0026#39;{__name__=~\u0026#34;node_.*|container_.*\u0026#34;}\u0026#39; 15 - \u0026#39;{__name__=~\u0026#34;windows_.*\u0026#34;}\u0026#39; 16 - \u0026#39;{__name__=~\u0026#34;probe_.*\u0026#34;}\u0026#39; 17 #- \u0026#39;{job!=\u0026#34;\u0026#34;}\u0026#39; 18 #- \u0026#39;{job=\u0026#34;node\u0026#34;}\u0026#39; 19 #- \u0026#39;{job=\u0026#34;blackbox_http_2xx\u0026#34;}\u0026#39; 20 #- \u0026#39;{job=\u0026#34;blackbox_icmp\u0026#34;}\u0026#39; 21 #match[]={__name__=~\u0026#34;..*\u0026#34;} 22 #match[]=\u0026#34;{__name__=~\u0026#34;.+\u0026#34;}\u0026#34; 23 static_configs: 24 - targets: 25 - \u0026#39;x.x.x.x:9090\u0026#39; 26 basic_auth: 27 username: \u0026#39;********\u0026#39; 28 password: \u0026#39;********\u0026#39; Sanity Check: You can connect to http://x.x.x.x:9090/targets and find the federation job status. Prometheus C pushes (forwards) metrics to Prometheus A In this scenario, you first need to enable the Remote Write Receiver feature in Prometheus A. You just need to add --web.enable-remote-write-receiver to your docker-compose.yml, no need to modify prometheus.yml\n1# prometheus.yml of A 2 prometheus: 3 image: prom/prometheus:latest 4 container_name: prometheus 5 expose: 6 - 9090 7 ports: 8 - 9090:9090 9 volumes: 10 - /opt/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro 11 - /opt/prometheus/web.yml:/etc/prometheus/web.yml:ro 12 command: 13 - \u0026#39;--config.file=/etc/prometheus/prometheus.yml\u0026#39; 14 - \u0026#39;--web.config.file=/etc/prometheus/web.yml\u0026#39; 15 - \u0026#39;--web.enable-remote-write-receiver\u0026#39; Then, you need to modify docker-compose.yml and *prometheus.yml of Prometheus C. First, you need to enable Prometheus agent mode, just need to add --enable-feature=agent to docker-compose.yml.\n1# docker-compose.yml of C 2 prometheus: 3 image: prom/prometheus:latest 4 container_name: prometheus 5 expose: 6 - 9090 7 ports: 8 - 9090:9090 9 volumes: 10 - /opt/grafana/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro 11 command: 12 - \u0026#39;--config.file=/etc/prometheus/prometheus.yml\u0026#39; 13 - \u0026#39;--enable-feature=agent\u0026#39; 14 - \u0026#39;--log.level=debug\u0026#39; Second, you need to edit prometheus.yml, add the remote_write section, and enter the URL of Prometheus A in it.\n1# prometheus.yml of C 2global: 3 scrape_interval: 15s 4 5remote_write: 6 - url: \u0026#39;http://(Domain of Prometheus A):9090/api/v1/write\u0026#39; 7 basic_auth: 8 username: \u0026#39;******\u0026#39; 9 password: \u0026#39;******\u0026#39; Sanity check: you can enable the log to debug level and check that it's sending the data.\n1prometheus | ts=2022-08-31T06:58:46.065Z caller=dedupe.go:112 component=remote level=debug remote_name=3c577a url=http://x.x.x.x:3128/api/v1/write msg=QueueManager.calculateDesiredShards dataInRate=95.10224000000001 dataOutRate=273.83119999999997 dataKeptRatio=1 dataPendingRate=-178.72895999999997 dataPending=285.30672000000004 dataOutDuration=0.026267519444000003 timePerSample=9.592595527463637e-05 desiredShards=0.010491189203871395 highestSent=1.66192912e+09 highestRecv=1.661929123e+09 Note You should enable authentication in Prometheus, you can refer to this page to hash your password https://prometheus.io/docs/guides/basic-auth/, and save it in your web.yml.\n1basic_auth_users: 2 username: ****************************************** Port forward If you have a restricted firewall, you can use socat to forward the request to another host.\n1socat -v TCP-LISTEN:3128,fork,reuseaddr TCP:x.x.x.x:9090 ","link":"https://dennys.github.io/en/doc/grafana/prometheus-pulls-pushes-metrics/","section":"doc","tags":["Prometheus"],"title":"Prometheus Pulls/Pushes Metrics from/to Another Prometheus"},{"body":"","link":"https://dennys.github.io/en/tags/openwrt/","section":"tags","tags":null,"title":"OpenWrt"},{"body":"Requirement Find a bandwidth management tool in OpenWrt to see which application (by interface) uses the most bandwidth.\nOpenWrt solution There are lots of bandwidth monitor tools in OpenWrt... https://openwrt.org/docs/guide-user/services/network_monitoring/bwmon#available_tools\ncollectd collectd is not just for network only, it's a common data collection tool in the UNIX platform. We can use collectd's network related plugins here.\nWebsite:\nOfficial website: https://openwrt.org/docs/guide-user/perf_and_log/statistic.collectd GitHub: https://github.com/collectd/collectd Installation: just install luci-app-statistics and it will include related libraries.\n1opkg install luci-app-statistics Usage\nConnect to Menu: Statistics -\u0026gt; Setup -\u0026gt; Network plugins, configure the plugin of the Interface or Wireless. Click Statistics -\u0026gt; Setup -\u0026gt; Graphs, and you can see the charts. vnStat: old brand, only for network Website:\nOfficial website: https://humdi.net/vnstat/ GitHub: https://github.com/vergoh/vnstat Installation\n1opkg install vnstat2 vnstati2 luci-app-vnstat2 Warning: vnstat1 and vnstat2 are not compatible, if you want to remove vnstat 1.x, please run the following commands:\n1opkg remove vnstat vnstati luci-app-vnstat 2rm /etc/vnstat.conf 3rm /etc/config/vnstat Usage\nConnect to http://192.168.1.1/cgi-bin/luci/admin/status/vnstat2 You can google some sample reports generated by vnStat https://www.google.com/search?q=vnstat+openwrt\u0026sxsrf=ALiCzsZM_jTIfojSOXzoTYU9g9BBB7h96A:1658385561017\u0026source=lnms\u0026tbm=isch\u0026sa=X\u0026ved=2ahUKEwiPhY_Sr4n5AhV-qFYBHWulCg8Q_AUoAXoECAEQAw\u0026biw=1920\u0026bih=969\u0026dpr=1 bandwidthd (stop maintenance) Website\nOfficial website(new): https://openwrt.org/docs/guide-user/services/network_monitoring/bandwidthd Official website(old): http://bandwidthd.sourceforge.net/ GitHub: https://github.com/NethServer/bandwidthd (stop maintenance) Installation: just follow the instructions of the official website\nUsage\nConnect to http://192.168.1.1/bandwidthd/ You can see the traffic of all IP addresses (The following charts are embedded from the official website) Note\nIt only provides network traffic by protocol, not enough. It runs several processes in the router, you can run top to check them. 1PID PPID USER STAT VSZ %VSZ %CPU COMMAND 24049 4046 root S 5220 4% 0% /usr/sbin/bandwidthd 34046 1 root S 5220 4% 0% /usr/sbin/bandwidthd 44048 4046 root S 5220 4% 0% /usr/sbin/bandwidthd 54050 4046 root S 5196 4% 0% /usr/sbin/bandwidthd ","link":"https://dennys.github.io/en/doc/router/openwrt-bandwidth-by-interface/","section":"doc","tags":["OpenWrt","改機","Router"],"title":"OpenWrt Bandwidth Management (by interface) "},{"body":"","link":"https://dennys.github.io/en/series/openwrt-%E6%94%B9%E6%A9%9F/","section":"series","tags":null,"title":"OpenWrt 改機"},{"body":"","link":"https://dennys.github.io/en/tags/router/","section":"tags","tags":null,"title":"Router"},{"body":"","link":"https://dennys.github.io/en/tags/%E6%94%B9%E6%A9%9F/","section":"tags","tags":null,"title":"改機"},{"body":"","link":"https://dennys.github.io/en/series/android-tips/","section":"series","tags":null,"title":"Android Tips"},{"body":"\rNotice\rThis solution is only for Android 11 or below!!! Please use Better Open With in Android 12 or above. Requirement Android's default YouTube App is not very convenient, there are lots of Open Source alternatives (YouTube Vanced (removed), NewPipe, ...), although you can install them, if you don't change the default YouTube App, it's still inconvenient.\nSolution Android Settings -\u0026gt; Apps -\u0026gt; Default apps -\u0026gt; Opening links\nIt will list all Apps, choose YouTube, and you will see the following screen. Disable \u0026quot;Open supported links\u0026quot;. Return to the previous menu, choose your preferred YouTube App, I use NewPipe, and choose * + Add link*. Enable YouTube related 4 links. Return to the previous menu, you can see NewPipe connects to the 4 links. FAQ If you find you cannot change it, you may forget to disable them on YouTube. Because a link can only link to 1 App. Result You can try to use Chrome to find some videos, try to open them and it will use NewPipe (or your preferred App) to open. ","link":"https://dennys.github.io/en/doc/android/android-modify-default-youtube-app/","section":"doc","tags":["Android","YouTube"],"title":"How to Change Default YouTube App (YouTube Vanced, NewPipe, ...) in Android"},{"body":"","link":"https://dennys.github.io/en/tags/youtube/","section":"tags","tags":null,"title":"YouTube"},{"body":"","link":"https://dennys.github.io/en/tags/grafana/","section":"tags","tags":null,"title":"Grafana"},{"body":"Requirement Usually we use SonarQube to check the KPI of projects: You can also check the build history of a project: Use Grafana to Manager SonarQube KPI But there are some SonarQube exporters, they can export SonarQube's KPI to Grafana/Prometheus.\nSolution 1 Exporter: https://github.com/dmeiners88/sonarqube-prometheus-exporter: It's a SonarQube plugin, but it only supports 5 KPIs, and the last commit date is 2018/11/9. It supports SonarQube 7, not sure if it supports 8 or 9, I don't plan to try it. Solution 2 Exporter: https://github.com/nthienan/sonarqube-exporter\nGrafana dashboard: you can find 2s dashboard overview.json and dashboard details.json on this GitHub.\nGenerate a SonarQube token, you can reference https://docs.sonarqube.org/latest/user-guide/user-token/\nHow to start this exporter\nYou can run this command directly:\n1docker run --expose 9102 -p 9102:9102 nthienan/sonarqube-exporter sqe -p 9102 --url http://sonar.x.x.x:9000 --user-token xxxxxxxxxxx --ignore-ssl-verification --log-level DEBUG If you use docker-compose, you can reference this docker-compose.yml\n1sonarqube-exporter: 2container_name: sonarqube-exporter 3image: nthienan/sonarqube-exporter 4ports: 5- 9102:9102 6expose: 7- 9102 8command: \u0026#39; sqe -p 9102 --url http://sonar.x.x.x:9000 --user-token xxxxxxxxxxx --ignore-ssl-verification --log-level DEBUG \u0026#39; 9environment: 10- TZ=Asia/Taipei 11depends_on: 12- sonarqube prometheus.yml\n1- job_name: \u0026#39;sonarqube\u0026#39; 2 scrape_interval: 5s 3 metrics_path: \u0026#39;/metrics\u0026#39; 4 static_configs: 5 - targets: [\u0026#39;xxx.xxx.xxx.xxx:9102\u0026#39;] Result Restart Prometheus and rebuild the project, and you can see the result in Grafana.\nYou can also check the detail dashboard, it contains more information like a build trend chart. The following is not a google sample, I'll update it in the future.\n","link":"https://dennys.github.io/en/doc/devops/grafana-sonarqube-integration/","section":"doc","tags":["SonarQube","CI/CD","Grafana"],"title":"Use Grafana to Manage SonarQube KPI"},{"body":"Change log 2022/10/13: Add some check points. Requirement When there are lots of projects on GitLab, it's not easy to know the overall build status of their CI/CD Pipelines.\nUse Grafana to Manage GitLab CI/CD Pipelines Solution 1: Use GitLab build-in metrics Exporter: GitLab build-in (reference: https://docs.gitlab.com/ee/administration/monitoring/prometheus/) Grafana dashboard: https://grafana.com/grafana/dashboards/10620 This dashboard needs this plugin: https://grafana.com/grafana/plugins/grafana-polystat-panel/ prometheus.yml 1- job_name: \u0026#39;GitLab\u0026#39; 2 scrape_interval: 5s 3 metrics_path: \u0026#39;/-/metrics\u0026#39; 4 scheme: https 5 static_configs: 6 - targets: [\u0026#39;gitlab.xxx.xxx.xxx\u0026#39;] 7 params: 8 token: [\u0026#39;***************\u0026#39;] Solution 2: Use gitlab-ci-pipelines-exporter Exporter: https://github.com/mvisonneau/gitlab-ci-pipelines-exporter This exporter provides several spectives metrics (with different Grafana dashboards) Grafana dashboard: the same as solution 1 (it also needs Ploystat plugin)\nHow to start gitlab-ci-pipelines-exporter\nAssume you put gitlab-ci-pipelines-exporter's configuration at /opt/grafana/gitlab-ci-pipelines-exporter/gitlab-ci-pipelines-exporter.yml\n1server: 2# [address:port] to make the process listen 3# upon (optional, default: :8080) 4listen_address: :9103 5gitlab: 6url: https://gitlab.xxx.xxx.xxx/ 7# You can also configure the token using --gitlab-token 8# or the $GCPE_GITLAB_TOKEN environment variable 9token: xrN14n9-ywvAFxxxxxx 10project_defaults: 11pull: 12 pipeline: 13 jobs: 14 enabled: true 15 environments: 16 enabled: true 17 18wildcards: 19- owner: 20 name: g1 21 kind: group 22 include_subgroups: true You can run this command to start this exporter in docker\n1docker run --restart always \\ 2--name gitlab-ci-pipelines-exporter \\ 3-v /opt/gitlab-runner/gitlab-ci-pipelines-exporter.yml:/etc/config.yml \\ 4-v /etc/hosts:/etc/hosts \\ 5--expose 9103 \\ 6-p 9103:9103 \\ 7mvisonneau/gitlab-ci-pipelines-exporter:v0.5.3 \\ 8run --config /etc/config.yml If you use docker-compose, you can reference the following yml\n1gitlab-ci-pipelines-exporter: 2image: mvisonneau/gitlab-ci-pipelines-exporter:v0.5.3 3container_name: gitlab-ci-pipelines-exporter 4ports: 5- 9103:9103 6expose: 7- 9103 8volumes: 9- /opt/grafana/gitlab-ci-pipelines-exporter/gitlab-ci-pipelines-exporter.yml:/etc/gitlab-ci-pipelines-exporter.yml 10- /etc/hosts:/etc/hosts 11environment: 12 GCPE_CONFIG: /etc/gitlab-ci-pipelines-exporter.yml 13 #GCPE_GITLAB_TOKEN: ${GCPE_GITLAB_TOKEN} 14 #GCPE_INTERNAL_MONITORING_LISTENER_ADDRESS: tcp://127.0.0.1:8082 How to check: You can connect to http://your.server:9103/metrics, and you should see some metrics start like \u0026quot;gitlab_ci_pineline_xxxx\u0026quot; If you only see metrics like \u0026quot;gcpe_projects_***\u0026quot; and \u0026quot;promhttp_metric_handler_errors_total\u0026quot;, you may need to check your configuration.\n1# TYPE gitlab_ci_pipeline_coverage gauge 2gitlab_ci_pipeline_coverage{kind=\u0026#34;branch\u0026#34;,project=\u0026#34;******\u0026#34;,ref=\u0026#34;master\u0026#34;,topics=\u0026#34;\u0026#34;,variables=\u0026#34;\u0026#34;} 0 3gitlab_ci_pipeline_coverage{kind=\u0026#34;branch\u0026#34;,project=\u0026#34;******\u0026#34;,ref=\u0026#34;master\u0026#34;,topics=\u0026#34;\u0026#34;,variables=\u0026#34;\u0026#34;} 0 4# HELP gitlab_ci_pipeline_duration_seconds Duration in seconds of the most recent pipeline 5# TYPE gitlab_ci_pipeline_duration_seconds gauge 6gitlab_ci_pipeline_duration_seconds{kind=\u0026#34;branch\u0026#34;,project=\u0026#34;******\u0026#34;,ref=\u0026#34;master\u0026#34;,topics=\u0026#34;\u0026#34;,variables=\u0026#34;\u0026#34;} 253 7gitlab_ci_pipeline_duration_seconds{kind=\u0026#34;branch\u0026#34;,project=\u0026#34;******\u0026#34;,ref=\u0026#34;master\u0026#34;,topics=\u0026#34;\u0026#34;,variables=\u0026#34;\u0026#34;} 489 8# HELP gitlab_ci_pipeline_id ID of the most recent pipeline 9# TYPE gitlab_ci_pipeline_id gauge 10gitlab_ci_pipeline_id{kind=\u0026#34;branch\u0026#34;,project=\u0026#34;******\u0026#34;,ref=\u0026#34;master\u0026#34;,topics=\u0026#34;\u0026#34;,variables=\u0026#34;\u0026#34;} 811 11gitlab_ci_pipeline_id{kind=\u0026#34;branch\u0026#34;,project=\u0026#34;******\u0026#34;,ref=\u0026#34;main\u0026#34;,topics=\u0026#34;\u0026#34;,variables=\u0026#34;\u0026#34;} 823 12...... prometheus.yml\n1- job_name: \u0026#39;gitlab-runner-ci-pipeline\u0026#39; 2 metrics_path: \u0026#39;/metrics\u0026#39; 3 scrape_interval: 5s 4 scheme: http 5 bearer_token: bearer_token 6 static_configs: 7 - targets: [\u0026#39;xxx.xxx.xxx.xxx:9103\u0026#39;] Result\nSolution 1 and 2 use the same dashboard, like the following: And you can click the ID, it will forward to GitLab's CI/CD Pipelines, you can see every jobs' execution status. Job status of CI Pipelines The above solution is to check the entire CI pipeline results, if you want to see individual job status in Grafana, you can modify gitlab-ci-pipelines-exporter.yml to add lines 5~6. And you can install this Grafana dashboard https://grafana.com/grafana/dashboards/13328\n1# Pull jobs related metrics on all projects 2project_defaults: 3pull: 4 pipeline: 5 jobs: 6 enabled: true Result\nYou can see every job's execution time and result, and you can click ID to see the detail in GitLab. Environments / Deployments status I don't try it yet, you can reference https://grafana.com/grafana/dashboards/13329\n","link":"https://dennys.github.io/en/doc/devops/grafana-gitlab-integration/","section":"doc","tags":["GitLab","CI/CD","Grafana"],"title":"Use Grafana to Manage GitLab CI/CD Pipelines"},{"body":"K-9 Mail 6.200 is released on 2022/07/08, it add the support to OAuth 2.0. Before it, if you activated the 2-factor verification on your Google account, when you try to use K-9 mail to connect to Gmail, you need to use App Passwords. It's not convenient, you can reference https://ekiwi-blog.de/en/16841/ to check the detail with a step-by-step video.\nIn K-9 Mail 6.200 release, it supports for using OAuth 2.0 with Google, Yahoo, AOL, and personal Microsoft accounts (Office365 accounts are not supported yet). You don't need to input your Google password anymore, you just need to grant privilege to K-9 mail.\n","link":"https://dennys.github.io/en/posts/202207/k-9-mail-supports-oauth-2.0-gmail/","section":"posts","tags":["Software"],"title":"K-9 Mail 6.200 Supports OAuth 2.0 (gmail)"},{"body":"","link":"https://dennys.github.io/en/posts/","section":"posts","tags":null,"title":"Posts"},{"body":"Requirement Most cameras' filenames are like 'DC*****' or 'P*****', I want to find a tool that has the following features.\nBatch change filenames to P[photo capture date]-[camera model].jpg. For example, P20220710-101405-iPhone 13.jpg. Can change the camera model string, for example, I want to change \u0026quot;iPhone 6\u0026quot; to \u0026quot;i6\u0026quot;, \u0026quot;DMC-G6\u0026quot; to \u0026quot;G6\u0026quot;, \u0026quot;DSC-RX100M5A\u0026quot; to \u0026quot;R1005A\u0026quot;, ... Can add a serial number automatically when the new filename is duplicated with others. Software There are lots of rename tools. I try 3 tools, and all of them are freeware. (Rename Master is not open source, but it's not a problem.). F2 (by Go and ExifTool (by Perl) are cross-platform. Rename Master is for Windows only.\nF2 (GitHub: 480 stars) ExifTool (GitHub: 1.5K stars) Rename Master F2 First run, use its build-in function, the problem is it cannot get the capture date.\n1C:\\\u0026gt;f2 -r \u0026#39;P{{mtime.YYYY}}{{mtime.MM}}{{mtime.DD}}-{{mtime.H}}{{mtime.mm}}{{mtime.ss}}-{{x.model}}\u0026#39; 2┌─────────────────────────────────────────────────┐ 3| ORIGINAL | RENAMED | STATUS | 4| *********************************************** | 5| P1790419.JPG | P20220528-154122-DMC-G6 | ok | 6| P1790420.JPG | P20220528-154134-DMC-G6 | ok | 7| P1790421.JPG | P20220528-154140-DMC-G6 | ok | 8└─────────────────────────────────────────────────┘ 9INFO Use the -x or --exec flag to apply the above changes Second run, use ExitTool to get the capture date. Because the date format of DateTimeOriginal is something like 2003:10:31 15:44:19, it contains several commons and it's an invalid character in Windows file system.\n1C:\\\u0026gt;f2 -r \u0026#39;P{{xt.DateTimeOriginal}}-{{x.model}}\u0026#39; 2┌──────────────────────────────────────────────────────────────────────────────────────┐ 3| ORIGINAL | RENAMED | STATUS | 4| ************************************************************************************ | 5| P1790419.JPG | \u0026#39;P2022:06:25 10:47:16-DMC-G6\u0026#39; | invalid characters present: (:,:,:,:) | 6| P1790419.JPG | \u0026#39;P2022:06:25 10:47:21-DMC-G6\u0026#39; | invalid characters present: (:,:,:,:) | 7| P1790419.JPG | \u0026#39;P2022:06:25 10:47:27-DMC-G6\u0026#39; | invalid characters present: (:,:,:,:) | 8└──────────────────────────────────────────────────────────────────────────────────────┘ 9ERROR Resolve conflicts before proceeding or use the -F flag to auto fix all conflicts Third run, in ExifTool, you can change the format of DateTimeOriginal, it seems we cannot do it in F2. Therefore, please add a -F parameter, it will try to fix invalid characters. In this case, it removes all invalid characters.\n1C:\\\u0026gt;f2 -r \u0026#39;P{{xt.DateTimeOriginal}}-{{x.model}}\u0026#39; -F 2┌───────────────────────────────────────────────────┐ 3| ORIGINAL | RENAMED | STATUS | 4| ************************************************* | 5| P1790419.JPG | \u0026#39;P20220528 154122-DMC-G6\u0026#39; | ok | 6| P1790420.JPG | \u0026#39;P20220528 154135-DMC-G6\u0026#39; | ok | 7| P1790421.JPG | \u0026#39;P20220528 154140-DMC-G6\u0026#39; | ok | 8└───────────────────────────────────────────────────┘ 9INFO Use the -x or --exec flag to apply the above changes I stop my test here... Because although F2 is very fast, but F2+ExifTool is much slower than F2 only, I guess it's due to F2 calls ExifTool externally. I try to create a feature request to add the support for DateTimeOriginal, I hope the project owner will accept it: https://github.com/ayoisaiah/f2/issues/23\nExifTool First run, use Exif tool to rename files, because the Exif function is build-in, the peformance is faster than F2+ExifTool. 1C:\\\u0026gt;exiftool \u0026#34;-filename\u0026lt;${CreateDate}-${Exif:Model}.${filetype}\u0026#34; -d P%Y%m%d-%H%M%S%%-c * 2 3C:\\\u0026gt;dir 42022/05/28 下午 03:41 8,024,576 P20220528-154122-DMC-G6.JPEG 52022/05/28 下午 03:41 6,673,920 P20220528-154135-DMC-G6.JPEG 62022/05/28 下午 03:41 7,978,496 P20220528-154140-DMC-G6.JPEG I cannot find a parameter to show the result, I need to run dir to check the new filenames. If you know how to do it, please let me know, thanks. I cannot find a function to modify the camera model string in ExifTool, but it's possible to do it by F2 easily. But for my requirement, I need to run F2 many times like the following. It should be ok but I prefer to find a better solution. Run ExifTool to rename files Run F2 to rename files for my 1st camera/phone. Run F2 to rename files for my 2nd camera/phone. Run F2 to rename files for my 3rd camera/phone. ... Rename Master Finally, I try to use Rename Master, it's a Windows GUI application, you can add actions (add string, find and replace, add serial number, ...). This is my script, it has 5 steps:\nRemove all filename. Add filename with a 'P' character and date+time. Add camera model into the filename. Change the name of camera model (ex: \u0026quot;DSC-RX100M2\u0026quot; -\u0026gt; \u0026quot;S2\u0026quot;, \u0026quot;iPhone 6 Plus\u0026quot; to \u0026quot;i6\u0026quot;) If the filenames are duplicated, add serian number in last. If you have hundreds of files, it will hang for several seconds (depending on your files) to scan the file, please be patient!\nThe following is my script, if you like, you can download it into a xxx.rmscr and use Rename Master to open it. Then you can modify the script for your requirement.\n1[SCRIPT0] 2TYPE_unicode=7 3ENABLED_unicode=1 4cbAndStore_unicode=0 5cbRCReplace_unicode=0 6cbxRemoveCharactersPart_unicode=0 7cbxRemoveCharPosition_unicode=0 8txtR2X_unicode=50 9txtRCReplace_unicode= 10 11[SCRIPT1] 12TYPE_unicode=0 13ENABLED_unicode=1 14cbOnCollision_unicode=0 15cbxAddAtThe_unicode=0 16cbxAddPart_unicode=0 17txtAdd_unicode=P?x_dtd:FYYYYMMDD?-?x_dtd:FHHMMSS?- 18 19[SCRIPT2] 20TYPE_unicode=0 21ENABLED_unicode=1 22cbOnCollision_unicode=0 23cbxAddAtThe_unicode=1 24cbxAddPart_unicode=0 25txtAdd_unicode=?x_mo? 26 27[SCRIPT3] 28TYPE_unicode=3 29ENABLED_unicode=1 30cbReplaceOnly_unicode=0 31cbxReplacePart_unicode=2 32cbxReplaceTarget_unicode=0 33txtReplacePhrase_unicode=DSC-RX100M5A 34txtReplaceWith_unicode=S5A 35txtRP2X_unicode=1 36txtRP2Y_unicode=10 37 38[SCRIPT7] 39TYPE_unicode=3 40ENABLED_unicode=1 41cbReplaceOnly_unicode=0 42cbxReplacePart_unicode=2 43cbxReplaceTarget_unicode=0 44txtReplacePhrase_unicode=iPhone 6 Plus 45txtReplaceWith_unicode=i6 46txtRP2X_unicode=1 47txtRP2Y_unicode=10 48 49[SCRIPT12] 50TYPE_unicode=0 51ENABLED_unicode=1 52cbOnCollision_unicode=1 53cbxAddAtThe_unicode=1 54cbxAddPart_unicode=0 55txtAdd_unicode=-?n02? 56 57[Path] 58cbPath_unicode= 59 60[Filter] 61txtFilter_unicode=*.jpg 62 63[ScriptOptions] 64UseFilter_unicode=1 65UsePath_unicode=0 66 67[Version] 68Current_unicode=3.9 69 70[Counting] 71rbUsePositionInList_unicode=1 72rbStartCountingAt_unicode=0 73txtStartCountingAt_unicode=1 74txtIncrementBy_unicode=1 75 76[Case] 77cbOverrideCase_unicode=0 78rbCUppercase_unicode=1 79rbCLowercase_unicode=0 80rbCCapitalizeWords_unicode=0 81rbCCapitalizeSentence_unicode=0 82cbCPreserverExtension_unicode=0 83rbCTitleCase_unicode=0 84cbCRoman_unicode=0 85cbSplit_unicode=0 86txtSplit_unicode= 87 88[Wildcards] 89rbWildcards_unicode=1 90rbRegExp_unicode=0 91 92[Text] 93txtTName_unicode= 94udTSkip_unicode=0 95cbTDelim_unicode=1 96rbTMatch_unicode=1 97rbTFind_unicode=0 98txtTFind_unicode=?cfn? 99txtTDelim_unicode=, 100cbxSearchWith_unicode=0 Conclusion Other sofotwares There are lots of rename tools, you can try them...\nBulk Rename Utility: it seems this utility has lots of features and it supports writing javascript to rename files. But it's for personal use only, if you want to use it in commercial, you need to pay. Ant Renamer: The last change is 2015/04/07. Ken Rename: I cannot find its official website of it. ","link":"https://dennys.github.io/en/doc/software/batch-photo-files-rename-tools/","section":"doc","tags":["Software"],"title":"Batch Photo Files Rename Tools (by EXIF)."},{"body":"I usually use Spotify to listen to podcasts, but because I use Spotify with my children, I want to find another Podcast App for my personal use.\nAnd I find Pocket Casts has an interesting feature, it can skip the first or last *** seconds. It means some podcasts have an introduction at the beginning, if you want to skip it, you can do it in this app. (This feature is provided in the free version.)\nSteps:\nChoose your podcast. Click the gear icon. Input the length you want to skip in the first or last. ","link":"https://dennys.github.io/en/doc/software/how-to-skip-podcast-intro/","section":"doc","tags":["Software","Podcast"],"title":"How to Skip Podcast's Introduction"},{"body":"","link":"https://dennys.github.io/en/tags/podcast/","section":"tags","tags":null,"title":"Podcast"},{"body":"","link":"https://dennys.github.io/en/tags/database/","section":"tags","tags":null,"title":"Database"},{"body":"If you want to get the 1st record of range records, you can use this SQL command.\n1SELECT * FROM 2(SELECT DISTINCT rank() over(PARTITION BY a.username ORDER BY a.update_dt desc) rn, a.* 3 FROM table_name a 4 WHERE a.update_dt \u0026gt; p_cur_timestamp 5 AND a.update_dt \u0026lt;= p_cur_timestamp + C_PERIOD 6 ) 7WHERE rn = 1 ","link":"https://dennys.github.io/en/doc/database/get-first-record-of-a-range-sql/","section":"doc","tags":["Database"],"title":"SQL to get 1st record of a range"},{"body":"This site uses Utterances to manage comments. If you want to contact me, you can leave a message here. Because Utterances is based on GitHub, you need to login GitHub, thanks.\n","link":"https://dennys.github.io/en/comments/","section":"","tags":null,"title":"Comments"},{"body":"Family \u0026#x1f440;\n\u0026#x1f468;\u0026#x1f469; \u0026#x1f467;\u0026#x1f467;\u0026#x1f467; Job \u0026#x1f468;\u0026zwj;\u0026#x1f4bb;\nLike \u0026#x1f436; \u0026#x1f431;\nHave \u0026#x1f430; \u0026#x1f407;\n","link":"https://dennys.github.io/en/about/","section":"","tags":null,"title":"About"},{"body":"","link":"https://dennys.github.io/en/categories/devops/","section":"categories","tags":null,"title":"DevOps"},{"body":"\rRemind\rThis is for .NET 4.8 or below only!!\rPrecondition You already have a GitLab (GitLab Saas or GitLab self-managed). This document is for .NET 4.X, if you want to build .NET core or .NET 5/6/7/..., you can use docker to run GitLab Runner.\nProcedure Step 1 - Prepare .NET build environment First, you need to prepare an environment to build .NET application. The easiest solution is to install Visual Studio. (Please let me know if I can build a .NET build environment without Visual Studio, thanks.) But it's strange, MSBuild.exe is not included in PATH after the installation. Please put C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Current\\Bin into your PATH. Step 2 - Install Git client Download and install Git for Windows. The portable version is ok, but because Git Runner is a Windows service, please add git.exe into PATH.\nStep 3 - Install GitLab Runner Download GitLab Runner for Windows.\nRename gitlab-runner-windows-amd64.exe to gitlab-runner.exe.\nGet the token to run GitLab Runner.\nGenerate GitLab Runner configuration file (config.toml) and run the command to register.\n1gitlab-runner.exe register --url https://gitlab.com/ --registration-token $REGISTRATION_TOKEN Modify config.toml\nBecause Gitlab Runner doesn't support Windows shell after version 13, you need to use PowerShell, you have 2 options: If you use Power Shell, please open config.toml and rename 'pwsh' to 'powershell'. If you use Power Shell Core, you don't need to do anything. If you find your log is garbled (ex: your Windows server is not an English version), please add chcp 65001 to change the encoding to UTF-8. If you use a portable Git, you need to modify $env:Path in config.toml. (comment line 13 and uncomment line 14) 1listen_address = \u0026#34;[::]:9252\u0026#34; 2concurrent = 4 3check_interval = 0 4 5[session_server] 6 session_timeout = 1800 7[[runners]] 8 name = \u0026#34;Windows Runner 01 (not docker)\u0026#34; 9 url = \u0026#34;https://gitlab.com/\u0026#34; 10 token = \u0026#34;******************\u0026#34; 11 executor = \u0026#34;shell\u0026#34; 12 shell = \u0026#34;powershell\u0026#34; 13 pre_clone_script = \u0026#34; chcp 65001\\n $OutputEncoding = [console]::InputEncoding = [console]::OutputEncoding = New-Object System.Text.UTF8Encoding\\n \u0026#34; 14 # pre_clone_script = \u0026#34; chcp 65001\\n $env:Path = \\\u0026#34;C:\\\\Gitlab\\\\PortableGit\\\\cmd;$env:Path\\\u0026#34;\\n\\t $OutputEncoding = [console]::InputEncoding = [console]::OutputEncoding = New-Object System.Text.UTF8Encoding\\n \u0026#34; 15 pre_build_script = \u0026#34; chcp 65001\\n \u0026#34; 16 [runners.custom_build_dir] 17 [runners.cache] 18 [runners.cache.s3] 19 [runners.cache.gcs] 20 [runners.cache.azure] Register GitLab Runner as a Windows service and start it.\nRun the command gitlab-runner install to install as a Windows service. Run the command gitlab-runner start to start the service. remember, if you modify the file, please execute gitlab-runner.exe restart to restart GitLab runner.\nIf you always see the Credential Helper Selector, please choose \u0026quot;no helper\u0026quot; and \u0026quot;Always use this from now on\u0026quot;.\nStep 4 - Write .gitlab-ci.yml Edit .gitlab-ci.yml, the following is a sample: 1stages: 2 - build 3 - test 4build: 5 stage: build 6 script: 7 - \u0026#34;dotnet build\u0026#34; 8 artifacts: 9 paths: 10 - .\\test 11test: 12 stage: test 13 script: 14 - \u0026#34;dotnet test\u0026#34; Step 5 - Start to build/test/deploy code (on local machine) Before you start to run CI/CD on your GitLab, you can try it on your local machine. It's easier to debug in a local environment. Please change the directory to the location of .gitlab-ci.yml and execute this command. 1C:\\Gitlab\\builds\\j5AFD9Qz\\0\\gis\\commission_moi_dtm\u0026gt; gitlab-runner.exe exec shell build 2Runtime platform arch=amd64 os=windows pid=13728 revision=e91107dd version=14.5.2 3Running with gitlab-runner 14.5.2 (e91107dd) 4Preparing the \u0026#34;shell\u0026#34; executor 5Using Shell executor... 6executor not supported job=1 project=0 referee=metrics 7Preparing environment 8Running on GITRUNNER01... 9DEPRECATION: CMD shell is deprecated and will no longer be supported 10Getting source from Git repository 11Fetching changes... 12Initialized empty Git repository in C:/Gitlab/builds/j5AFD9Qz/0/gis/**********/builds/0/project-0/.git/ 13Created fresh repository. 14Checking out 1ff057d4 as HEAD... 15... Step 6 - Start to test CI/CD (on GitLab) If everything is ok, you can commit .gitlab-ci.yml and GitLab should run it automatically.\nSAST (Static Application Security Testing) GitLab can check your source code for known vulnerabilities, unfortunately, it only supports Linux containers, and Windows containers are not yet supported. (reference: https://docs.gitlab.com/ee/user/application_security/sast/)\n","link":"https://dennys.github.io/en/doc/devops/gitlab-dotnet4-ci-cd/","section":"doc","tags":["GitLab",".NET","CI/CD"],"title":"Use GitLab to do .NET 4.8 CI/CD"},{"body":"","link":"https://dennys.github.io/en/tags/ifttt/","section":"tags","tags":null,"title":"IFTTT"},{"body":"Requirement I want to have daily mail including currency exchange rate information.\nMail subject: include the exchange rate in the mail subject. Mail content: Has some links to banks' exchange rate pages. Include some exchange rate history charts. Solution Use ifttt.com\nCreate a new Applet on IFTTT\nAdd a new \u0026quot;If This\u0026quot;\nChoose \u0026quot;finance\u0026quot; service\nChoose \u0026quot;Today's exchange rate report\u0026quot;\nChoose the input/output currency and trigger time.\nAdd a new \u0026quot;Then That\u0026quot;\nChoose a \u0026quot;gmail\u0026quot; service\nIf you just want to send yourself an email, you can click the right button. If you want to send to several recipients, you can click left button.\nThis is an example to send yourself an email.\nThis is my email body\n1As of {{CheckTime}}, 1 {{InputCurrency}} equals {{ExchangeRate}} {{OutputCurrency}}.\u0026lt;br\u0026gt; 2\u0026lt;br\u0026gt; 3via {{InfoUrl}} 4 5\u0026lt;b\u0026gt;美元匯率:\u0026lt;/b\u0026gt; 6\u0026lt;ul\u0026gt;\u0026lt;li\u0026gt; 7\u0026lt;a href=\u0026#34;https://rate.bot.com.tw/xrt?Lang=zh-TW\u0026#34;\u0026gt;即時\u0026lt;/a\u0026gt;(台銀)\u0026amp;nbsp; 8\u0026lt;a href=\u0026#34;https://rate.bot.com.tw/xrt/quote/day/USD\u0026#34;\u0026gt;當日\u0026lt;/a\u0026gt;(台銀)\u0026amp;nbsp; 9\u0026lt;a href=\u0026#34;https://www.xe.com/currencycharts/?from=USD\u0026amp;to=TWD\u0026amp;view=1W\u0026#34;\u0026gt;1周\u0026lt;/a\u0026gt;(XE)\u0026amp;nbsp; 10\u0026lt;a href=\u0026#34;https://www.xe.com/currencycharts/?from=USD\u0026amp;to=TWD\u0026amp;view=1M\u0026#34;\u0026gt;1個月\u0026lt;/a\u0026gt;(XE)\u0026amp;nbsp; 11\u0026lt;a href=\u0026#34;https://rate.bot.com.tw/xrt/quote/ltm/USD\u0026#34;\u0026gt;3個月\u0026lt;/a\u0026gt;(台銀)\u0026amp;nbsp; 12\u0026lt;a href=\u0026#34;https://rate.bot.com.tw/xrt/quote/l6m/USD\u0026#34;\u0026gt;6個月\u0026lt;/a\u0026gt;(台銀) 13\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt; 14\u0026lt;a href=\u0026#34;https://rate.bot.com.tw/xrt/quote/l1y/USD\u0026#34;\u0026gt;1年\u0026lt;/a\u0026gt;(台銀)\u0026amp;nbsp; 15\u0026lt;a href=\u0026#34;https://www.esunbank.com.tw/bank/personal/deposit/rate/forex/exchange-rate-chart?Currency=USD/TWD\u0026#34;\u0026gt;1年\u0026lt;/a\u0026gt;(玉山)\u0026amp;nbsp; 16\u0026lt;a href=\u0026#34;https://www.xe.com/currencycharts/?from=USD\u0026amp;to=TWD\u0026amp;view=1Y\u0026#34;\u0026gt;1年\u0026lt;/a\u0026gt;(XE)\u0026amp;nbsp; 17\u0026lt;a href=\u0026#34;https://rate.bot.com.tw/xrt/quote/l3y/USD\u0026#34;\u0026gt;3年\u0026lt;/a\u0026gt;(台銀)\u0026amp;nbsp; 18\u0026lt;a href=\u0026#34;https://www.xe.com/currencycharts/?from=USD\u0026amp;to=TWD\u0026amp;view=5Y\u0026#34;\u0026gt;5年\u0026lt;/a\u0026gt;(XE)\u0026amp;nbsp; 19\u0026lt;a href=\u0026#34;https://www.xe.com/currencycharts/?from=USD\u0026amp;to=TWD\u0026amp;view=10Y\u0026#34;\u0026gt;10年\u0026lt;/a\u0026gt;(XE)\u0026amp;nbsp; 20\u0026lt;a href=\u0026#34;https://fxtop.com/en/historical-exchange-rates.php?A=1\u0026amp;C1=USD\u0026amp;C2=TWD\u0026amp;DD1=\u0026amp;MM1=\u0026amp;YYYY1=\u0026amp;B=1\u0026amp;P=\u0026amp;I=1\u0026amp;DD2=03\u0026amp;MM2=03\u0026amp;YYYY2=2099\u0026amp;btnOK=Go%21\u0026#34;\u0026gt;1983~現在\u0026lt;/a\u0026gt;(FXTOP)\u0026amp;nbsp; 21\u0026lt;/li\u0026gt;\u0026lt;li\u0026gt; 22\u0026lt;a href=\u0026#34;https://historical.findrate.tw/USD/\u0026#34;\u0026gt;1個月+1年+10年\u0026lt;/a\u0026gt;(FindRate) 23\u0026lt;/li\u0026gt;\u0026lt;/ul\u0026gt;\u0026lt;br/\u0026gt; 24 25\u0026lt;b\u0026gt;2022/1/1~2022/12/31:\u0026lt;/b\u0026gt;\u0026lt;br/\u0026gt; 26\u0026lt;img src=\u0026#34;https://fxtop.com/php/imggraph.php?C1=USD\u0026amp;C2=TWD\u0026amp;A=1\u0026amp;DD1=\u0026amp;MM1=01\u0026amp;YYYY1=2022\u0026amp;DD2=\u0026amp;MM2=12\u0026amp;YYYY2=2022\u0026amp;LANG=en\u0026amp;CJ=0\u0026amp;MM1Y=1\u0026amp;LARGE=\u0026amp;TR=OFF\u0026#34;\u0026gt;\u0026lt;br/\u0026gt; 27\u0026lt;b\u0026gt;2021/1/1~2022/12/31:\u0026lt;/b\u0026gt;\u0026lt;br/\u0026gt; 28\u0026lt;img src=\u0026#34;https://fxtop.com/php/imggraph.php?C1=USD\u0026amp;C2=TWD\u0026amp;A=1\u0026amp;DD1=\u0026amp;MM1=01\u0026amp;YYYY1=2021\u0026amp;DD2=\u0026amp;MM2=12\u0026amp;YYYY2=2022\u0026amp;LANG=en\u0026amp;CJ=0\u0026amp;MM1Y=1\u0026amp;LARGE=\u0026amp;TR=OFF\u0026#34;\u0026gt;\u0026lt;br/\u0026gt; 29\u0026lt;b\u0026gt;2020/1/1~2022/12/31:\u0026lt;/b\u0026gt;\u0026lt;br/\u0026gt; 30\u0026lt;img src=\u0026#34;https://fxtop.com/php/imggraph.php?C1=USD\u0026amp;C2=TWD\u0026amp;A=1\u0026amp;DD1=\u0026amp;MM1=01\u0026amp;YYYY1=2020\u0026amp;DD2=\u0026amp;MM2=12\u0026amp;YYYY2=2022\u0026amp;LANG=en\u0026amp;CJ=0\u0026amp;MM1Y=1\u0026amp;LARGE=\u0026amp;TR=OFF\u0026#34;\u0026gt;\u0026lt;br/\u0026gt; 31\u0026lt;b\u0026gt;2000/1/1~2022/12/31:\u0026lt;/b\u0026gt;\u0026lt;br/\u0026gt; 32\u0026lt;img src=\u0026#34;https://fxtop.com/php/imggraph.php?C1=USD\u0026amp;C2=TWD\u0026amp;A=1\u0026amp;DD1=\u0026amp;MM1=01\u0026amp;YYYY1=2000\u0026amp;DD2=\u0026amp;MM2=12\u0026amp;YYYY2=2022\u0026amp;LANG=en\u0026amp;CJ=0\u0026amp;MM1Y=1\u0026amp;LARGE=\u0026amp;TR=OFF\u0026#34;\u0026gt;\u0026lt;br/ Mail sample IFTTT applet location This is my IFTTT applet: https://ifttt.com/applets/SxPJ2fpv-\nAbout the exchange rate history chart I only find 2 websites that provide exchange rate charts to embed in the mail content.\nhttps://fxtop.com/, it provides PNG format chart, I already include it in my IFTTT applet. https://www.currencyconverterrate.com/, it provides SVG format chart (ex: https://www.currencyconverterrate.com/currencycharts/usd/usd-twd-exchange-rates-history-chart-7-day.svg), but I cannot find a solution to show SVG in gmail. If you find a solution, please let me know, thanks. ","link":"https://dennys.github.io/en/doc/ifttt/ifttt-send-currency-exchange-rate-mail/","section":"doc","tags":["IFTTT"],"title":"Use IFTTT to send currency exchange rate mail daily"},{"body":"Requirement I want to have daily mail including currency exchange rate information.\nSolution use https://wise.com (no need to register an account)\nConnect to https://wise.com/tools/exchange-rate-alerts/ Input the currency exchange rate you want to know and your email address. Mail result You will receive the mail daily.\n","link":"https://dennys.github.io/en/doc/software/wise-send-currency-exchange-rate-mail/","section":"doc","tags":["Software"],"title":"Use WISE to send currency exchange rate mail daily"},{"body":"","link":"https://dennys.github.io/en/tags/checkmarx/","section":"tags","tags":null,"title":"Checkmarx"},{"body":"","link":"https://dennys.github.io/en/categories/ci/cd/","section":"categories","tags":null,"title":"CI/CD"},{"body":"\rPurpose Checkmarx is a good tool to do code analysis, but there are 2 pain points during the flow.\nCheckmarx can integrate SCM like GitHub/GitLab/..., but it cannot be triggered by by a code commit, it can only schedule the job to run the scan. (If I don't commit code, I don't want to waste resources scanning the code.) Checkmarx has a built-in issue tracking system, but I think most people prefer to use their issue tracking system in their Git or JIRA or ... What we want is to integrate Checkmarx scan into GitLab's CI/CD flow including\nWhen we commit code to GitLab, it will trigger Checkmarx to scan automatically. After the scan, Checkmarx will create issues in GitLab. Procedure GitLab admin configuration You need to set some global variables in GitLab CI/CD admin (Menu-\u0026gt;Admin-\u0026gt;Settings-\u0026gt;CI/CD-\u0026gt;Variables). You need to set CHECKMARX_XXX and CX_TEAM variables to integrate Checkmarx. And if you want Checkmarx to create GitLab issue, please also set GITLAB_URL and GITLAB_TOKEN. (You can reference here for the detail) GitLab project configuration It's very easy to enable Checkmarx analysis in your GitLab jobs, you just need to edit your .gitlab-ci.yml and include Checkmarx's yml into it (line 1). And you also need to assign Checkmarx's project name in this yml file (line 4).\n1include: \u0026#39;https://raw.githubusercontent.com/checkmarx-ltd/cx-flow/develop/templates/gitlab/v3/Checkmarx.gitlab-ci.yml\u0026#39; 2 3variables: 4 CX_PROJECT: ProjectXXX #The project name you want to show in Checkmarx Then you can try to commit code to trigger a CI/CD flow, Checkmarx analysis will be triggered in 2 conditions:\nWhen you create a merge request in GitLab. When you commit code to master stream directly. The following is a log sample, please check line 20, you can see the variables you defined in GitLab.\n1Running with gitlab-runner 14.5.2 (e91107dd) 2 on Runner in PC vYY9jCrR 3Preparing the \u0026#34;docker\u0026#34; executor 400:06 5Using Docker executor with image checkmarx/cx-flow ... 6Pulling docker image checkmarx/cx-flow ... 7Using docker image sha256:22f48d49aa64275d09b1650c359c0a9b1ab9fa771efa01fba4af89f511b93481 for checkmarx/cx-flow with digest checkmarx/cx-flow@sha256:2b2a2f09680e6c3ba21b00cf8ab4005baa133f0eeeaeb406f09b01a832fffb67 ... 8Preparing environment 900:01 10Running on runner-vyy9jcrr-project-33-concurrent-0 via 435566b8ed7c... 11Getting source from Git repository 1200:14 13Fetching changes... 14 15...... 16 17 18Executing \u0026#34;step_script\u0026#34; stage of the job script 19Using docker image sha256:22f48d49aa64275d09b1650c359c0a9b1ab9fa771efa01fba4af89f511b93481 for checkmarx/cx-flow with digest checkmarx/cx-flow@sha256:2b2a2f09680e6c3ba21b00cf8ab4005baa133f0eeeaeb406f09b01a832fffb67 ... 20$ ${CX_FLOW_EXE} --scan --app=\u0026#34;${CI_PROJECT_NAME}\u0026#34; --namespace=\u0026#34;${CI_PROJECT_NAMESPACE}\u0026#34; --repo-name=\u0026#34;${CI_PROJECT_NAME}\u0026#34; --repo-url=\u0026#34;${CI_REPOSITORY_URL}\u0026#34; --cx-team=\u0026#34;${CX_TEAM}\u0026#34; --cx-project=\u0026#34;${CX_PROJECT}\u0026#34; --branch=\u0026#34;${CI_COMMIT_BRANCH}\u0026#34; --spring.profiles.active=\u0026#34;${CX_FLOW_ENABLED_VULNERABILITY_SCANNERS}\u0026#34; --f=. ${PARAMS} 21SLF4J: Class path contains multiple SLF4J bindings. 22SLF4J: Found binding in [jar:file:/app/cx-flow.jar!/BOOT-INF/lib/logback-classic-1.2.3.jar!/org/slf4j/impl/StaticLoggerBinder.class] 23SLF4J: Found binding in [jar:file:/app/cx-flow.jar!/BOOT-INF/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] 24SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 25SLF4J: Actual binding is of type [ch.qos.logback.classic.util.ContextSelectorStaticBinder] 26 ___ ___ _ 27 / __\\_ __ / __\\ | _____ __ 28 / / \\ \\/ /____ / _\\ | |/ _ \\ \\ /\\ / / 29/ /___ \u0026gt; \u0026lt;_____/ / | | (_) \\ V V / 30\\____//_/\\_\\ \\/ |_|\\___/ \\_/\\_/ 312022-01-21 06:44:09.841 WARN 11 --- [ main] o.s.b.StartupInfoLogger [] : InetAddress.getLocalHost().getHostName() took 5006 milliseconds to respond. Please verify your network configuration. 322022-01-21 06:44:14.858 INFO 11 --- [ main] c.c.f.CxFlowApplication [] : Starting CxFlowApplication with PID 11 (/app/cx-flow.jar started by root in /builds/*********) 332022-01-21 06:44:14.858 INFO 11 --- [ main] c.c.f.CxFlowApplication [] : The following profiles are active: sast 342022-01-21 06:44:18.305 INFO 11 --- [ main] ptablePropertiesBeanFactoryPostProcessor [] : Post-processing PropertySource instances 352022-01-21 06:44:18.464 INFO 11 --- [ main] c.u.j.EncryptablePropertySourceConverter [] : Converting PropertySource configurationProperties [org.springframework.boot.context.properties.source.ConfigurationPropertySourcesPropertySource] to AOP Proxy 362022-01-21 06:44:18.467 INFO 11 --- [ main] c.u.j.EncryptablePropertySourceConverter [] : Converting PropertySource commandLineArgs [org.springframework.core.env.SimpleCommandLinePropertySource] to EncryptableEnumerablePropertySourceWrapper 372022-01-21 06:44:18.467 INFO 11 --- [ main] c.u.j.EncryptablePropertySourceConverter [] : Converting PropertySource systemProperties [org.springframework.core.env.PropertiesPropertySource] to EncryptableMapPropertySourceWrapper 382022-01-21 06:44:18.468 INFO 11 --- [ main] c.u.j.EncryptablePropertySourceConverter [] : Converting PropertySource systemEnvironment [org.springframework.boot.env.SystemEnvironmentPropertySourceEnvironmentPostProcessor$OriginAwareSystemEnvironmentPropertySource] to EncryptableSystemEnvironmentPropertySourceWrapper 392022-01-21 06:44:18.468 INFO 11 --- [ main] c.u.j.EncryptablePropertySourceConverter [] : Converting PropertySource random [org.springframework.boot.env.RandomValuePropertySource] to EncryptablePropertySourceWrapper 402022-01-21 06:44:18.469 INFO 11 --- [ main] c.u.j.EncryptablePropertySourceConverter [] : Converting PropertySource applicationConfig: [classpath:/application-sast.yml] [org.springframework.boot.env.OriginTrackedMapPropertySource] to EncryptableMapPropertySourceWrapper 412022-01-21 06:44:18.469 INFO 11 --- [ main] c.u.j.EncryptablePropertySourceConverter [] : Converting PropertySource applicationConfig: [classpath:/application.yml] [org.springframework.boot.env.OriginTrackedMapPropertySource] to EncryptableMapPropertySourceWrapper 422022-01-21 06:44:18.607 INFO 11 --- [ main] c.u.j.f.DefaultLazyPropertyFilter [] : Property Filter custom Bean not found with name \u0026#39;encryptablePropertyFilter\u0026#39;. Initializing Default Property Filter 432022-01-21 06:44:18.620 INFO 11 --- [ main] c.u.j.r.DefaultLazyPropertyResolver [] : Property Resolver custom Bean not found with name \u0026#39;encryptablePropertyResolver\u0026#39;. Initializing Default Property Resolver 442022-01-21 06:44:18.629 INFO 11 --- [ main] c.u.j.d.DefaultLazyPropertyDetector [] : Property Detector custom Bean not found with name \u0026#39;encryptablePropertyDetector\u0026#39;. Initializing Default Property Detector 452022-01-21 06:44:18.934 WARN 11 --- [ main] j.p.spi [] : javax.persistence.spi::No valid providers found. 462022-01-21 06:44:18.938 INFO 11 --- [ main] trationDelegate$BeanPostProcessorChecker [] : Bean \u0026#39;org.hibernate.validator.internal.constraintvalidators.bv.NotBlankValidator\u0026#39; of type [org.hibernate.validator.internal.constraintvalidators.bv.NotBlankValidator] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying) 472022-01-21 06:44:18.942 INFO 11 --- [ main] trationDelegate$BeanPostProcessorChecker [] : Bean \u0026#39;org.hibernate.validator.internal.constraintvalidators.bv.NotNullValidator\u0026#39; of type [org.hibernate.validator.internal.constraintvalidators.bv.NotNullValidator] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying) 482022-01-21 06:44:18.944 INFO 11 --- [ main] trationDelegate$BeanPostProcessorChecker [] : Bean \u0026#39;flowProperties\u0026#39; of type [com.checkmarx.flow.config.FlowProperties] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying) 492022-01-21 06:44:18.947 INFO 11 --- [ main] trationDelegate$BeanPostProcessorChecker [] : Bean \u0026#39;flowAsyncConfig\u0026#39; of type [com.checkmarx.flow.config.FlowAsyncConfig$$EnhancerBySpringCGLIB$$9e6d5344] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying) 502022-01-21 06:44:22.959 INFO 11 --- [ main] o.s.w.s.s.SaajSoapMessageFactory [] : Creating SAAJ 1.3 MessageFactory with SOAP 1.1 Protocol 512022-01-21 06:44:23.569 INFO 11 --- [ main] o.s.s.c.ThreadPoolTaskExecutor [] : Initializing ExecutorService 522022-01-21 06:44:23.571 INFO 11 --- [ main] o.s.s.c.ThreadPoolTaskExecutor [] : Initializing ExecutorService \u0026#39;scanRequest\u0026#39; 532022-01-21 06:44:23.576 INFO 11 --- [ main] o.s.s.c.ThreadPoolTaskExecutor [] : Initializing ExecutorService 542022-01-21 06:44:23.577 INFO 11 --- [ main] o.s.s.c.ThreadPoolTaskExecutor [] : Initializing ExecutorService \u0026#39;webHook\u0026#39; 552022-01-21 06:44:23.790 INFO 11 --- [ main] c.c.f.CxFlowRunner [] : =======BUILD INFO======= 562022-01-21 06:44:23.791 INFO 11 --- [ main] c.c.f.CxFlowRunner [] : Version: cx-flow-1.6.29 572022-01-21 06:44:23.792 INFO 11 --- [ main] c.c.f.CxFlowRunner [] : Time: 2022-01-07T12:45:49.239Z 582022-01-21 06:44:23.793 INFO 11 --- [ main] c.c.f.CxFlowRunner [] : ======================= 592022-01-21 06:44:25.426 INFO 11 --- [ main] c.c.f.CxFlowApplication [] : Started CxFlowApplication in 32.909 seconds (JVM running for 34.89) 602022-01-21 06:44:25.457 INFO 11 --- [ main] c.c.f.CxFlowRunner [blKXd4Ey] : Using custom bean implementation for bug tracking 612022-01-21 06:44:25.539 INFO 11 --- [ main] c.c.f.s.ScaFilterFactory [blKXd4Ey] : Initializing SCA filters. 622022-01-21 06:44:25.541 INFO 11 --- [ main] c.c.f.CxFlowRunner [blKXd4Ey] : Executing scan process 632022-01-21 06:44:25.551 INFO 11 --- [ main] c.c.f.s.ScaFilterFactory [blKXd4Ey] : Initializing SCA filters. 642022-01-21 06:44:25.571 INFO 11 --- [ main] c.c.f.s.ProjectNameGenerator [blKXd4Ey] : Project name being used: ********* 652022-01-21 06:44:25.573 INFO 11 --- [ main] c.c.f.u.ZipUtils [blKXd4Ey] : Creating zip file /builds/*********/cx.1357f707-5066-486c-b6f5-1d9dc08a4ad9.zip from contents of path . 662022-01-21 06:44:25.573 INFO 11 --- [ main] c.c.f.u.ZipUtils [blKXd4Ey] : Applying exclusions: .jar 672022-01-21 06:44:47.686 INFO 11 --- [ main] c.c.f.u.ZipUtils [blKXd4Ey] : Successfully created /builds/*********/cx.1357f707-5066-486c-b6f5-1d9dc08a4ad9.zip 682022-01-21 06:44:47.688 INFO 11 --- [ main] c.c.f.s.ScanRequestConverter [blKXd4Ey] : Overriding team with /CxServer/TEAM*** 692022-01-21 06:44:47.702 INFO 11 --- [ main] c.c.s.s.CxAuthService [blKXd4Ey] : Logging into Checkmarx https://***.com/cxrestapi/auth/identity/connect/token 702022-01-21 06:44:48.665 INFO 11 --- [ main] c.c.s.s.CxService [blKXd4Ey] : Retrieving Cx teams 712022-01-21 06:44:48.700 INFO 11 --- [ main] c.c.s.s.CxService [blKXd4Ey] : Found team /CxServer/TEAM*** with ID 16 722022-01-21 06:44:48.920 INFO 11 --- [ main] c.c.s.s.CxService [blKXd4Ey] : Creating scan... 732022-01-21 06:44:48.921 INFO 11 --- [ main] c.c.s.s.CxService [blKXd4Ey] : Updating Source details for project Id 67 742022-01-21 06:45:05.838 INFO 11 --- [ main] c.c.s.s.CxService [blKXd4Ey] : Scan will be Full Scan 752022-01-21 06:45:05.840 INFO 11 --- [ main] c.c.s.s.CxService [blKXd4Ey] : Creating Scan for project Id 67 762022-01-21 06:45:06.021 INFO 11 --- [ main] c.c.s.s.CxService [blKXd4Ey] : Scan created with Id 1000207 for project Id 67 772022-01-21 06:45:06.171 INFO 11 --- [ main] jsonLogger [blKXd4Ey] : 782022-01-21 07:22:12.549 INFO 11 --- [ main] jsonLogger [blKXd4Ey] : 792022-01-21 07:22:12.551 INFO 11 --- [ main] c.c.s.s.CxService [blKXd4Ey] : Creating report for xml Id 1000207 802022-01-21 07:22:12.906 INFO 11 --- [ main] c.c.s.s.CxService [blKXd4Ey] : Report with Id 304 created 812022-01-21 07:22:17.908 INFO 11 --- [ main] c.c.s.s.CxService [blKXd4Ey] : Retrieving report status of report Id 304 822022-01-21 07:22:18.965 INFO 11 --- [ main] c.c.s.s.CxAuthService [blKXd4Ey] : Logging into Checkmarx for SOAP token https://***.com/cxrestapi/auth/identity/connect/token 832022-01-21 07:22:19.117 INFO 11 --- [ main] c.c.s.s.CxService [blKXd4Ey] : Retrieving report contents of report Id 304 in XML format 842022-01-21 07:22:19.220 INFO 11 --- [ main] c.c.s.s.CxService [blKXd4Ey] : Report downloaded for report Id 304 852022-01-21 07:22:19.805 INFO 11 --- [ main] c.c.s.s.CxService [blKXd4Ey] : Fetching Scan data for Id 1000207 862022-01-21 07:22:19.820 INFO 11 --- [ main] c.c.s.s.CxService [blKXd4Ey] : Fetching custom fields from project ID 67 872022-01-21 07:22:19.908 INFO 11 --- [ main] c.c.f.s.ResultsService [blKXd4Ey] : Issue tracking is custom bean implementation 882022-01-21 07:22:19.923 INFO 11 --- [ main] c.c.f.c.GitLabIssueTracker [blKXd4Ey] : Initializing GitLab processing 892022-01-21 07:22:20.325 INFO 11 --- [ main] c.c.f.s.CodeBashingService [blKXd4Ey] : not using CodeBashing lessons integration - one or more of the mandatory properties is missing 902022-01-21 07:22:20.325 INFO 11 --- [ main] c.c.f.s.IssueService [blKXd4Ey] : Processing Issues with custom bean GitLab 912022-01-21 07:22:20.325 INFO 11 --- [ main] c.c.f.c.GitLabIssueTracker [blKXd4Ey] : Executing getIssues GitLab API call 922022-01-21 07:22:20.465 INFO 11 --- [ main] c.c.f.s.IssueService [blKXd4Ey] : Issue still exists. Updating issue with key CX Second_Order_SQL_Injection @ ***.cs [master] 932022-01-21 07:22:21.352 INFO 11 --- [ main] c.c.f.c.GitLabIssueTracker [blKXd4Ey] : Finalizing GitLab Processing 942022-01-21 07:22:21.352 INFO 11 --- [ main] c.c.f.s.ResultsService [blKXd4Ey] : ####Checkmarx Scan Results Summary#### 952022-01-21 07:22:21.352 INFO 11 --- [ main] c.c.f.s.ResultsService [blKXd4Ey] : Team: /CxServer/TEAM***, Project: *********, Scan-Id: 1000207 962022-01-21 07:22:21.353 INFO 11 --- [ main] c.c.f.s.ResultsService [blKXd4Ey] : The vulnerabilities found for the scan are: high: 2, medium: 21, low: 156, info: 0 972022-01-21 07:22:21.353 INFO 11 --- [ main] c.c.f.s.ResultsService [blKXd4Ey] : To view results use following link: https://***.com/CxWebClient/ViewerMain.aspx?scanid=1000207\u0026amp;projectid=67 982022-01-21 07:22:21.353 INFO 11 --- [ main] c.c.f.s.ResultsService [blKXd4Ey] : ###################################### 992022-01-21 07:22:21.353 INFO 11 --- [ main] c.c.f.s.ThresholdValidatorImpl [blKXd4Ey] : Checking Thresholds exists. sast thresholds: false. sca thresholds: false 1002022-01-21 07:22:21.353 INFO 11 --- [ main] c.c.f.CxFlowRunner [blKXd4Ey] : Build succeeded. all checks passed 1012022-01-21 07:22:21.353 INFO 11 --- [ main] c.c.f.CxFlowRunner [blKXd4Ey] : Completed Successfully 1022022-01-21 07:22:21.354 INFO 11 --- [ main] c.c.f.CxFlowRunner [blKXd4Ey] : Finished with exit code: 0 1032022-01-21 07:22:21.359 INFO 11 --- [extShutdownHook] o.s.s.c.ThreadPoolTaskExecutor [] : Shutting down ExecutorService \u0026#39;webHook\u0026#39; 1042022-01-21 07:22:21.362 INFO 11 --- [extShutdownHook] o.s.s.c.ThreadPoolTaskExecutor [] : Shutting down ExecutorService \u0026#39;scanRequest\u0026#39; 105Cleaning up project directory and file based variables 10600:05 107Job succeeded During the Checkmarx analysis, you can check the status from its GUI, it takes a long time to analyze.\nAfter the analysis, you can check the result in Checkmarx or GitLab.\nCheck the result in Checkmarx.\nCheck the result in GitLab, it generates 1 report in merge request and it will create several issues by category. The following is the reqport of a merge request. In this report, there are 9 high issues (in 5 categories). The following are the 5 issues created by Checkmarx in GitLab. You can click above issue, it shows the detail vulnerability information. Troubleshooting If you find Checkmarx cannot find your team, please make sure to use English only in CX_TEAM, if you use non-English (ex: Chinese or Japanese), you can check the log and find it cannot find the team. 12022-01-12 02:51:22.748 INFO 11 --- [main] c.c.s.s.CxService [x4DNMhOL] : Found team /CxServer/Team1 with ID 16 Reference You can reference the official document to do the integration: https://checkmarx.atlassian.net/wiki/spaces/SD/pages/1929937052/GitLab+Integration. Conclusion After this integration, you just need to commit the code and see the result in GitLab. All flows are in GitLab now, you don't need to worry about Checkmarx. Checkmarx provides lots of integration, you can reference https://checkmarx.atlassian.net/wiki/spaces/SD/pages/1339162785/CxSAST+CxSCA+Plugins+and+Integrations. What's next Try to integrate the scan result to SonarQube), you can reference https://dennys.github.io/en/doc/devops/gitlab-checkmarx-sonarqube-integration/. ","link":"https://dennys.github.io/en/doc/devops/gitlab-checkmarx-integration/","section":"doc","tags":["GitLab","Checkmarx"],"title":"GitLab Checkmarx CI/CD Integration"},{"body":"\rPurpose In previous article, we know how to integrate GitLab and Checkmarx. In this article, we want to integrate SonarQube too. When we commit code to GitLab, we want GitLab to trigger these actions automatically:\nGitLab sends the code to Checkmarx to scan. GitLab triggers SonarQube to scan. SonarQube integrates Checkmarx's report. Procedure You can reference the official document: https://checkmarx.atlassian.net/wiki/spaces/SD/pages/169246832/SonarQube+Plugin+v8.5.0+and+up\nDownload the plugin from here, it only supports SonarQube LTS version (for now, it's 8.x) Configurea Quality Gate/Profiles of SonarQube for Checkmarx's rules. Use GitLab to trigger Checkmarx scan and record the project name of Checkmarx. Configure Checkmarx data in SonarQube, which you can reference here. Trigger GitLab CI again, you will see the following log in your SonarQube job 1INFO: Sensor Import Checkmarx scan results to SonarQube [checkmarx] 2INFO: Retrieving Checkmarx scan results for current module [Checkmarx plugin version: 2021.2.1] 3INFO: Getting Checkmarx configuration data from sonar Database. 4INFO: Resolving Cx setting: checkmarx.server.project_name 5INFO: Forced authentication is enabled: Sonar credentials must be provided 6INFO: Sonar server token is provided 7INFO: Checkmarx credentials migration not needed 8INFO: Sonar server token is provided 9INFO: Resolving Cx setting: checkmarx.server.project_name 10INFO: Forced authentication is enabled: Sonar credentials must be provided 11INFO: Checkmarx server version [9.2.0.41015]. Hotfix [24]. 12INFO: Logging into the Checkmarx service. 13INFO: Connecting to https://your.checkmarx.server/ 14INFO: Initializing Cx client [2020.2.4.NO.SCA] 15INFO: Checkmarx server version [9.2.0.41015]. Hotfix [24]. 16INFO: Logging into the Checkmarx service. 17INFO: full team path: \\CxServer\\\\Team1 18INFO: preset name: All 19INFO: ---------------------------------Get Last CxSAST Results:-------------------------------- 20INFO: Waiting for server to generate xml report. 4990 seconds left to timeout 21INFO: Checkmarx High vulnerabilities: 3 22INFO: Checkmarx New-High vulnerabilities: 0 23INFO: Checkmarx Medium vulnerabilities: 23 24INFO: Checkmarx New-Medium vulnerabilities: 1 25INFO: Checkmarx Low vulnerabilities: 142 26INFO: Checkmarx New-Low vulnerabilities: 7 27INFO: Checkmarx scan link: https://your.checkmarx.server//CxWebClient/ViewerMain.aspx?scanId=1000157\u0026amp;ProjectID=67 You can see the Checkmarx issues in SonarQube now. Problem I find some issues are not created in SonarQube and it seems due to the rule is not defined in Checkmarx's SonarQube plugin. I'm still checking it.\n","link":"https://dennys.github.io/en/doc/devops/gitlab-checkmarx-sonarqube-integration/","section":"doc","tags":["GitLab","Checkmarx","SonarQube"],"title":"GitLab Checkmarx SonarQube Integration"},{"body":"Check table size (remember to check last_analyzed_date) 1SELECT * FROM all_tables WHERE table_name LIKE \u0026#39;AB%\u0026#39; Re-analyze table 1ANALYZE table TABLE_NAME estimate statisitics Check index-related information (it doesn't contain block information) 1SELECT * FROM all_indexes WHERE table_name LIKE \u0026#39;AB%\u0026#39; Re-analyze index: 1ANALYZE index INDEX_NAME validate structure; Check index size: (it only checks the last analyzed index) 1SELECT * FROM index_stats; Others An index needs to be rebuilt, deleting or updating records will NOT free the index block Need DBA to check the block's definition. ","link":"https://dennys.github.io/en/doc/database/how-to-analyze-size-of-table-index/","section":"doc","tags":["Database"],"title":"How to analyze size of table, index in Oracle"}]