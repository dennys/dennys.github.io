<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on Dennys Diary</title>
    <link>http://localhost:1313/en/tags/ai/</link>
    <description>Recent content in AI on Dennys Diary</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Copyright © 2008–2018, Steve Francia and the Hugo Authors; all rights reserved.</copyright>
    <lastBuildDate>Thu, 31 Oct 2024 00:00:00 +0800</lastBuildDate><atom:link href="http://localhost:1313/en/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ollama &#43; Open Web UI</title>
      <link>http://localhost:1313/en/doc/ai/ollama-open-webui/</link>
      <pubDate>Thu, 31 Oct 2024 00:00:00 +0800</pubDate>
      
      <guid>http://localhost:1313/en/doc/ai/ollama-open-webui/</guid>
      <description>
        
          
            xxx [Ollama] https://github.com/ollama/ollama [Open WebUI] https://github.com/open-webui/open-webui Docker commands This is the docker-compse.yml for Ollama and Open Web UI, you can use docker-compose up -d to start these containers.
You can use 1$ cat docker-compose.yml 2version: &amp;#39;3.8&amp;#39; 3services: 4 ollama: 5 image: ollama/ollama:0.3.13 6 expose: 7 - 11434 8 ports: 9 - 11434:11434 10 volumes: 11 - .:/code 12 - ./ollama/ollama:/root/.ollama 13 container_name: ollama 14 #pull_policy: always 15 tty: true 16 restart: always 17 networks: 18 - ollama-docker 19 deploy: 20 resources: 21 reservations: 22 devices: 23 - driver: nvidia 24 device_ids: [&amp;#39;0&amp;#39;, &amp;#39;1&amp;#39;, &amp;#39;2&amp;#39;] # Use specific GPU 25# count: 1 26# count: all # Use all GPUs 27 capabilities: [gpu] 28 29 open-webui: 30 image: ghcr.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
